[
  {
    "path": "posts/2021-04-13-using-kubernetes-and-the-future-package-to-easily-parallelize-r-in-the-cloud/",
    "title": "Using Kubernetes and the Future Package to Easily Parallelize R in the Cloud ",
    "description": "Useful details on paralellising with Google Cloud",
    "author": [
      {
        "name": "Chris Paciorek",
        "url": "https://www.jottr.org/2021/04/08/future-and-kubernetes/"
      }
    ],
    "date": "2021-04-13",
    "categories": [],
    "contents": "\nIn this post, I’ll demonstrate that you can easily use the future package in R on a cluster of machines running in the cloud, specifically on a Kubernetes cluster.\nThis allows you to easily doing parallel computing in R in the cloud. One advantage of doing this in the cloud is the ability to easily scale the number and type of (virtual) machines across which you run your parallel computation.\nWhy use Kubernetes to start a cluster in the cloud?\nKubernetes is a platform for managing containers. You can think of the containers as lightweight Linux machines on which you can do your computation. By using the Kubernetes service of a cloud provider such as Google Cloud Platform (GCP) or Amazon Web Services (AWS), you can easily start up a cluster of (virtual) machines.\nThere have been (and are) approaches to starting up a cluster of machines on AWS easily from the command line on your laptop. Some tools that are no longer actively maintained are StarCluster and CfnCluster. And there is now something called AWS ParallelCluster. But doing it via Kubernetes allows you to build upon an industry standard platform that can be used on various cloud providers. A similar effort (which I heavily borrowed from in developing the setup described here) allows one to run a Python Dask cluster accessed via a Jupyter notebook.\nMany of the cloud providers have Kubernetes services (and it’s also possible you’d have access to a Kubernetes service running at your institution or company). In particular, I’ve experimented with Google Kubernetes Engine (GKE) and Amazon’s Elastic Kubernetes Service (EKS). This post will demonstrate setting up your cluster using Google’s GKE, but see my GitHub future-kubernetes repository for details on doing it on Amazon’s EKS. Note that while I’ve gotten things to work on EKS, there have been various headaches that I haven’t encountered on GKE.\nI’m not a Kubernetes expert, nor a GCP or AWS expert (that might explain the headaches I just mentioned), but one upside is that hopefully I’ll go through all the details at a level someone who is not an expert can follow along. In fact, part of my goal in setting this up has been to learn more about Kubernetes, which I’ve done, but note that there’s a lot to it.\nMore details about the setup, including how it was developed and troubleshooting tips can be found in my future-kubernetes repository.\nHow it works (briefly)\nThis diagram in Figure 1 outlines the pieces of the setup.\n Figure 1. Overview of using future on a Kubernetes cluster\nWork on a Kubernetes cluster is divided amongst pods, which carry out the components of your work and can communicate with each other. A pod is basically a Linux container. (Strictly speaking a pod can contain multiple containers and shared resources for those containers, but for our purposes, it’s simplest just to think of a pod as being a Linux container.) The pods run on the nodes in the Kubernetes cluster, where each Kubernetes node runs on a compute instance of the cloud provider. These instances are themselves virtual machines running on the cloud provider’s actual hardware. (I.e., somewhere out there, behind all the layers of abstraction, there are actual real computers running on endless aisles of computer racks in some windowless warehouse!) One of the nice things about Kubernetes is that if a pod dies, Kubernetes will automatically restart it.\nThe basic steps are:\nStart your Kubernetes cluster on the cloud provider’s Kubernetes service\nStart the pods using Helm, the Kubernetes package manager\nConnect to the RStudio Server session running on the cluster from your browser\nRun your future-based computation\nTerminate the Kubernetes cluster\nWe use the Kubernetes package manager, Helm, to run the pods of interest:\none (scheduler) pod for a main process that runs RStudio Server and communicates with the workers\nmultiple (worker) pods, each with one R worker process to act as the workers managed by the future package\nHelm manages the pods and related services. An example of a service is to open a port on the scheduler pod so the R worker processes can connect to that port, allowing the scheduler pod RStudio Server process to communicate with the worker R processes. I have a Helm chart that does this; it borrows heavily from the Dask Helm chart for the Dask package for Python.\nEach pod runs a Docker container. I use my own Docker container that layers a bit on top of the Rocker container that contains R and RStudio Server.\nStep 1: Start the Kubernetes cluster\nHere I assume you have already installed:\nthe command line interface to Google Cloud,\nthe kubectl interface for interacting with Kubernetes, and\nhelm for installing Helm charts (i.e., Kubernetes packages).\nInstallation details can be found in the future-kubernetes repository.\nFirst we’ll start our cluster (the first part of Step 1 in Figure 1):\ngcloud container clusters create \\\n    --machine-type n1-standard-1 \\\n    --num-nodes 4 \\\n    --zone us-west1-a \\\n    --cluster-version latest \\\n    my-cluster\nI’ve asked for four virtual machines (nodes), using the basic (and cheap) n1-standard-1 instance type (which has a single CPU per virtual machine) from Google Cloud Platform.\nYou’ll want to specify the total number of cores on the virtual machines to be equal to the number of R workers that you want to start and that you specify in the Helm chart (as discussed below). Here we ask for four one-cpu nodes, and our Helm chart starts four workers, so all is well. See the Modifications section below on how to start up a different number of workers.\nSince the RStudio Server process that you interact with wouldn’t generally be doing heavy computation at the same time as the workers, it’s OK that the RStudio scheduler pod and a worker pod would end up using the same virtual machine.\nStep 2: Install the Helm chart to set up your pods\nNext we need to get our pods going by installing the Helm chart (i.e., package) on the cluster; the installed chart is called a release. As discussed above, the Helm chart tells Kubernetes what pods to start and how they are configured.\nFirst we need to give our account permissions to perform administrative actions:\nkubectl create clusterrolebinding cluster-admin-binding \\\n    --clusterrole=cluster-admin\nNow let’s install the release. This code assumes the use of Helm version 3 or greater (for older versions see my full instructions).\ngit clone https://github.com/paciorek/future-helm-chart   # download the materials\ntar -czf future-helm.tgz -C future-helm-chart .           # create a zipped archive (tarball) that `helm install` needs\nhelm install --wait test ./future-helm.tgz                # install (start the pods)\nYou’ll need to name your release; I’ve used ‘test’ above.\nThe --wait flag tells helm to wait until all the pods have started. Once that happens, you’ll see a message about the release and how to connect to the RStudio interface, which we’ll discuss further in the next section.\nWe can check the pods are running:\nkubectl get pods\nYou should see something like this (the alphanumeric characters at the ends of the names will differ in your case):\nNAME                                READY   STATUS    RESTARTS   AGE\nfuture-scheduler-6476fd9c44-mvmz6   1/1     Running   0          116s\nfuture-worker-54db85cb7b-47qsd      1/1     Running   0          115s\nfuture-worker-54db85cb7b-4xf4x      1/1     Running   0          115s\nfuture-worker-54db85cb7b-rj6bj      1/1     Running   0          116s\nfuture-worker-54db85cb7b-wvp4n      1/1     Running   0          115s\nAs expected, we have one scheduler and four workers.\nStep 3: Connect to RStudio Server running in the cluster\nNext we’ll connect to the RStudio instance running via RStudio Server on our main (scheduler) pod, using the browser on our laptop (Step 3 in Figure 1).\nAfter installing the Helm chart, you should have seen a printout with some instructions on how to do this. First you need to connect a port on your laptop to the RStudio port on the main pod (running of course in the cloud):\nexport RSTUDIO_SERVER_IP=\"127.0.0.1\"\nexport RSTUDIO_SERVER_PORT=8787\nkubectl port-forward --namespace default svc/future-scheduler $RSTUDIO_SERVER_PORT:8787 &\nYou can now connect from your browser to the RStudio Server instance by going to the URL: https://127.0.0.1:8787.\nEnter rstudio as the username and future as the password to login to RStudio.\nWhat’s happening is that port 8787 on your laptop is forwarding to the port on the main pod on which RStudio Server is listening (which is also port 8787). So you can just act as if RStudio Server is accessible directly on your laptop.\nOne nice thing about this is that there is no public IP address for someone to maliciously use to connect to your cluster. Instead the access is handled securely entirely through kubectl running on your laptop. However, it also means that you couldn’t easily share your cluster with a collaborator. For details on configuring things so there is a public IP, please see my repository.\nNote that there is nothing magical about running your computation via RStudio. You could connect to the main pod and simply run R in it and then use the future package.\nStep 4: Run your future-based parallel R code\nNow we’ll start up our future cluster and run our computation (Step 4 in Figure 1):\nlibrary(future)\nplan(cluster, manual = TRUE, quiet = TRUE)\nThe key thing is that we set manual = TRUE above. This ensures that the functions from the future package don’t try to start R processes on the workers, as those R processes have already been started by Kubernetes and are waiting to connect to the main (RStudio Server) process.\nNote that we don’t need to say how many future workers we want. This is because the Helm chart sets an environment variable in the scheduler pod’s Renviron file based on the number of worker pod replicas. Since that variable is used by the future package (via parallelly::availableCores()) as the default number of future workers, this ensures that there are only as many future workers as you have worker pods. However, if you modify the number of worker pods after installing the Helm chart, you may need to set the workers argument to plan() manually. (And note that if you were to specify more future workers than R worker processes (i.e., pods) you would get an error and if you were to specify fewer, you wouldn’t be using all the resources that you are paying for.)\nNow we can use the various tools in the future package as we would if on our own machine or working on a Linux cluster.\nLet’s run our parallelized operations. I’m going to do the world’s least interesting calculation of calculating the mean of many (10 million) random numbers forty separate times in parallel. Not interesting, but presumably if you’re reading this you have your own interesting computation in mind and hopefully know how to do it using future’s tools such as future.apply and foreach with doFuture.\nlibrary(future.apply)\noutput <- future_sapply(1:40, function(i) mean(rnorm(1e7)), future.seed = TRUE)\nNote that all of this assumes you’re working interactively, but you can always reconnect to the RStudio Server instance after closing the browser, and any long-running code should continue running even if you close the browser.\nFigure 2 shows a screenshot of the RStudio interface.\n Figure 2. Screenshot of the RStudio interface\nWorking with files\nNote that /home/rstudio will be your default working directory in RStudio and the RStudio Server process will be running as the user rstudio.\nYou can use /tmp and /home/rstudio for files, both within RStudio and within code running on the workers, but note that files (even in /home/rstudio) are not shared between workers nor between the workers and the RStudio Server pod.\nTo make data available to your RStudio process or get output data back to your laptop, you can use kubectl cp to copy files between your laptop and the RStudio Server pod. Here’s an example of copying to/from /home/rstudio:\n## create a variable with the name of the scheduler pod\nexport SCHEDULER=$(kubectl get pod --namespace default -o jsonpath='{.items[?(@.metadata.labels.component==\"scheduler\")].metadata.name}')\n\n## copy a file to the scheduler pod\nkubectl cp my_laptop_file ${SCHEDULER}:home/rstudio/\n\n## copy a file from the scheduler pod\nkubectl cp ${SCHEDULER}:home/rstudio/my_output_file .\nOf course you can also interact with the web from your RStudio process, so you could download data to the RStudio process from the internet.\nStep 5: Cleaning up\nMake sure to shut down your Kubernetes cluster, so you don’t keep getting charged.\ngcloud container clusters delete my-cluster --zone=us-west1-a\nModifications\nYou can modify the Helm chart in advance, before installing it. For example you might want to install other R packages for use in your parallel code or change the number of workers.\nTo add additional R packages, go into the future-helm-chart directory (which you created using the directions above in Step 2) and edit the values.yaml file. Simply modify the lines that look like this:\n  env:\n  #  - name: EXTRA_R_PACKAGES\n  #    value: data.table\nby removing the “#” comment characters and putting the R packages you want installed in place of data.table, with the names of the packages separated by spaces, e.g.,\n  env:\n    - name: EXTRA_R_PACKAGES\n      value: foreach doFuture\nIn many cases you may want these packages installed on both the scheduler pod (where RStudio Server runs) and on the workers. If so, make sure to modify the lines above in both the scheduler and worker stanzas.\nTo modify the number of workers, modify the replicas line in the worker stanza of the values.yaml file.\nThen rebuild the Helm chart:\ncd future-helm-chart  ## ensure you are in the directory containing `values.yaml`\ntar -czf ../future-helm.tgz .\nand install as done previously.\nNote that doing the above to increase the number of workers would probably only make sense if you also modify the number of virtual machines you start your Kubernetes cluster with such that the total number of cores across the cloud provider compute instances matches the number of worker replicas.\nYou may also be able to modify a running cluster. For example you could use gcloud container clusters resize. I haven’t experimented with this.\nTo modify if your Helm chart is already installed (i.e., your release is running), one simple option is to reinstall the Helm chart as discussed below. You may also need to kill the port-forward process discussed in Step 3.\nFor some changes, you can also also update a running release without uninstalling it by “patching” the running release or scaling resources. I won’t go into details here.\nTroubleshooting\nThings can definitely go wrong in getting all the pods to start up and communicate with each other. Here are some suggestions for monitoring what is going on and troubleshooting.\nFirst, you can use kubectl to check the pods are running:\nkubectl get pods\nConnect to a pod\nTo connect to a pod, which allows you to check on installed software, check on what the pod is doing, and other troubleshooting, you can do the following\nexport SCHEDULER=$(kubectl get pod --namespace default -o jsonpath='{.items[?(@.metadata.labels.component==\"scheduler\")].metadata.name}')\nexport WORKERS=$(kubectl get pod --namespace default -o jsonpath='{.items[?(@.metadata.labels.component==\"worker\")].metadata.name}')\n\n## access the scheduler pod:\nkubectl exec -it ${SCHEDULER}  -- /bin/bash\n## access a worker pod:\necho $WORKERS\nkubectl exec -it <insert_name_of_a_worker> -- /bin/bash\nAlternatively just determine the name of the pod with kubectl get pods and then run the kubectl exec -it ... invocation above.\nNote that once you are in a pod, you can install software in the usual fashion of a Linux machine (in this case using apt commands such as apt-get install).\nConnect to a virtual machine\nOr to connect directly to an underlying VM, you can first determine the name of the VM and then use the gcloud tools to connect to it.\nkubectl get nodes\n## now, connect to one of the nodes, 'gke-my-cluster-default-pool-8b490768-2q9v' in this case:\ngcloud compute ssh gke-my-cluster-default-pool-8b490768-2q9v --zone us-west1-a\nCheck your running code\nTo check that your code is actually running in parallel, one can run the following test and see that the result returns the names of distinct worker pods.\nlibrary(future.apply)\nfuture_sapply(seq_len(nbrOfWorkers()), function(i) Sys.info()[[\"nodename\"]])\nYou should see something like this:\n[1] future-worker-54db85cb7b-47qsd future-worker-54db85cb7b-4xf4x\n[3] future-worker-54db85cb7b-rj6bj future-worker-54db85cb7b-wvp4n\nOne can also connect to the pods or to the underlying virtual nodes (as discussed above) and run Unix commands such as top and free to understand CPU and memory usage.\nReinstall the Helm release\nYou can restart your release (i.e., restarting the pods, without restarting the whole Kubernetes cluster):\nhelm uninstall test\nhelm install --wait test ./future-helm.tgz \nNote that you may need to restart the entire Kubernetes cluster if you’re having difficulties that reinstalling the release doesn’t fix.\nHow does it work?\nI’ve provided many of the details of how it works in my future-kubernetes repository.\nThe key pieces are:\nThe Helm chart with the instructions for how to start the pods and any associated services.\nThe Rocker-based Docker container(s) that the pods run.\nThat’s all there is to it … plus these instructions.\nBriefly:\nBased on the Helm chart, Kubernetes starts up the ‘main’ or ‘scheduler’ pod running RStudio Server and multiple worker pods each running an R process. All of the pods are running the Rocker-based Docker container\nThe RStudio Server main process and the workers use socket connections (via the R function\nsocketConnection()\n) to communicate:\nthe worker processes start R processes that are instructed to regularly make a socket connection using a particular port on the main scheduler pod\nwhen you run future::plan() (which calls makeClusterPSOCK()) in RStudio, the RStudio Server process attempts to make socket connections to the workers using that same port\n\nOnce the socket connections are established, command of the RStudio session returns to you and you can run your future-based parallel R code.\nOne thing I haven’t had time to work through is how to easily scale the number of workers after the Kubernetes cluster is running and the Helm chart installed, or even how to auto-scale – starting up workers as needed based on the number of workers requested via plan().\nWrap up\nIf you’re interested in extending or improving this or collaborating in some fashion, please feel free to get in touch with me via the ‘future-kubernetes’ issue tracker or by email.\nAnd if you’re interested in using R with Kubernetes, note that RStudio provides an integration of RStudio Server Pro with Kubernetes that should allow one to run future-based workflows in parallel.\n/Chris\nLinks\nfuture-kubernetes repository:\nGitHub page: https://github.com/paciorek/future-kubernetes\n\nfuture-kubernetes Helm chart:\nGitHub page: https://github.com/paciorek/future-helm-chart\n\nfuture-kubernetes Docker container:\nGitHub page: https://github.com/paciorek/future-kubernetes-docker\n\nfuture package:\nCRAN page: https://cran.r-project.org/package=future\nGitHub page: https://github.com/HenrikBengtsson/future\n\n\n",
    "preview": {},
    "last_modified": "2021-04-13T13:52:40+10:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-04-13-using-rselenium-to-scrape-a-paginated-html-table/",
    "title": "Using RSelenium to scrape a paginated HTML table",
    "description": "RSelenium code snippet really",
    "author": [
      {
        "name": "Guillaume Pressiat",
        "url": "https://guillaumepressiat.github.io/blog/2021/04/RSelenium-paginated-tables"
      }
    ],
    "date": "2021-04-13",
    "categories": [],
    "contents": "\nTrying to answer this question on stackoverflow about understat.com scraping I was interested to take RSelenium for a spin.\nFew years ago, Selenium and R weren’t particularly friends (Python+Selenium were more used for instance) but it seems to have changed. Package author and rOpenSci works and documentation did it.\nAfter few tries with xpath spellings, I have found RSelenium pretty nice actually. I share here some recipes in this context: when you want to scrape a paginated table that is not purely HTML but a result of embedded javascript execution in browser.\nA thing that wans’t particularly easy in Selenium at the beginning was how to extract sub-elements like html table code and not “source page as a whole”. I have used innerHTML attribute for this.\nThis example explains how emulate clicks can be done to navigate from elements to others in the HTML page, and a more focus point on moving from page to page in a paginated table.\nHere is a youtube video with subtitles I have made to illustrate it (no voice).\nFirst step to follow is to download a selenium-server-xxx.jar file here, see this vignette.\nand run in the terminal : java -jar selenium-server-standalone-xxx.jar\nthen you can inspect precisely elements of the HTML page code in browser and go back and forth between RStudio and the emulated browser (right click, inspect element)\nat the end use rvest to parse html tables\nfor instance find an id like league-chemp that we are using with RSelenium:\ncapture htmlelem_chemp <- remDr$findElement(using=\"xpath\", value=\"//*[@id='league-chemp']\").\nHere is a gist/snippets on github.\nAlso see the gist embedded below.\n# https://stackoverflow.com/q/67021563/10527496\n\n\n# java -jar selenium-server-standalone-3.9.1.jar \n\n\nlibrary(RSelenium)\nlibrary(tidyverse)\nlibrary(rvest)\nlibrary(httr)\n\nremDr <- remoteDriver(\n  remoteServerAddr = \"localhost\",\n  port = 4444L, # change port according to terminal \n  browserName = \"firefox\"\n)\n\nremDr$open()\n# remDr$getStatus()\nremDr$navigate(\"https://understat.com/league/Ligue_1/\")\n\n\n# find championship table in html via xpath\nelem_chemp <- remDr$findElement(using=\"xpath\", value=\"//*[@id='league-chemp']\")\n\n# move to this table via script (optional)\nremDr$executeScript(\"arguments[0].scrollIntoView(true);\", args = list(elem_chemp))\n\n# scrape the html table as a tibble\nresults_champ <- read_html(elem_chemp$getElementAttribute('innerHTML')[[1]]) %>% \n  html_table() %>% .[[1]] %>% \n  slice(-1)\n\n\n# find player table in html via xpath\nelem_player_page_number <- remDr$findElement(using=\"xpath\", value=\"//*[@id='league-players']\")\n# find it using html id directly\n# elem_player_page_number <- remDr$findElement(using=\"id\", value = \"league-players\")\n\n# find number of pages of this paginated table\nplayer_page_number <- read_html(elem_player_page_number$getElementAttribute('innerHTML')[[1]]) %>% \n  html_nodes('li.page') %>% \n  html_attr('data-page') %>% \n  as.integer() %>% \n  max()\n\n\n# move to this table via script\nremDr$executeScript(\"arguments[0].scrollIntoView(true);\", args = list(elem_player_page_number))\n\n# or scroll at the bottom of page\n# body_b <- remDr$findElement(\"css\", \"body\")\n# body_b$sendKeysToElement(list(key = \"end\"))\n# then you can go to top\n# body_b$sendKeysToElement(list(key = \"home\"))\n\n\ni <- 4\none_table_at_a_time <- function(i){\n  # move on the desired page\n  \n  elem_click <- remDr$findElement('xpath', \n                                  glue::glue('//*[@id=\"league-players\"]\n                                             //*[normalize-space(@data-page) = \"{i}\"]'))\n  remDr$mouseMoveToLocation(webElement = elem_click)\n  elem_click$click()\n  \n  # get the table for 10 players\n  elem_player <- remDr$findElement(using=\"xpath\", value=\"//*[@id='league-players']\")\n  results_player <- read_html(elem_player$getElementAttribute('innerHTML')[[1]]) %>% \n    html_table()\n  \n  message('Player table scraped, page ', i)\n  results_player %>% \n    .[[1]] %>% \n    filter(!is.na(Apps)) %>% \n    return()\n  \n}\n\n# one_table_at_a_time(3) %>% View\n# loop over pages\nresu <- 1:player_page_number %>% purrr::map_df(one_table_at_a_time)\n\n\n\n",
    "preview": {},
    "last_modified": "2021-04-13T13:58:47+10:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-03-25-framework-for-power-analysis-using-simulation/",
    "title": "Framework for power analysis using simulation",
    "description": "A short description of the post.",
    "author": [
      {
        "name": "Keith Goldfeld",
        "url": "https://www.rdatagen.net/post/2021-03-16-framework-for-power-analysis-using-simulation/"
      }
    ],
    "date": "2021-03-25",
    "categories": [],
    "contents": "\nThe simstudy package started as a collection of functions I developed as I found myself repeating many of the same types of simulations for different projects. It was a way of organizing my work that I decided to share with others in case they wanted a routine way to generate data as well. simstudy has expanded a bit from that, but replicability is still a key motivation.\nWhat I have here is another attempt to document and organize a process that I find myself doing quite often - repeated data generation and model fitting. Whether I am conducting a power analysis using simulation or exploring operating characteristics of different models, I take a pretty similar approach. I refer to this structure when I am starting a new project, so I thought it would be nice to have it easily accessible online - and that way others might be able to refer to it as well.\nThe framework\nI will provide a simple application below, but first I’ll show the general structure. The basic idea is that we want to generate data under a variety of assumptions - for example, a power analysis will assume different sample sizes, effects, and/or levels of variation - and for each set of assumptions, we want to generate a large number of replications to mimic repeated sampling from a population. The key elements of the process include (1) defining the data, (2) generating a data set, (3) fitting a model to the data, and (4) providing summary statistics.\nIf you have familiarity with simstudy, I’d say the code is pretty self-explanatory. In the function s_generate, there is a call to base R function list2env, which makes all elements of a list available as variables in the function’s environment. The replication process is managed by the mclapply function from the parallel package. (Alternative approaches include using function lapply in base R or using a for loop.)\ns_define <- function() {\n  \n  #--- add data definition code ---#\n  \n  return(list_of_defs) # list_of_defs is a list of simstudy data definitions\n}\n\ns_generate <- function(list_of_defs, argsvec) {\n  \n  list2env(list_of_defs, envir = environment())\n  list2env(as.list(argsvec), envir = environment())\n  \n  #--- add data generation code ---#\n  \n  return(generated_data) #  generated_data is a data.table\n}\n\ns_model <- function(generated_data) {\n  \n  #--- add model code ---#\n  \n  return(model_results) # model_results is a data.table\n}\n\ns_single_rep <- function(list_of_defs, argsvec) {\n  \n  generated_data <- s_generate(list_of_defs, argsvec)\n  model_results <- s_model(generated_data)\n  \n  return(model_results)\n}\n\ns_replicate <- function(argsvec, nsim) {\n  \n  list_of_defs <- s_define()\n\n  model_results <- rbindlist(\n    parallel::mclapply(\n      X = 1 : nsim, \n      FUN = function(x) s_single_rep(list_of_defs, argsvec), \n      mc.cores = 4)\n  )\n  \n  #--- add summary statistics code ---#\n  \n  return(summary_stats) # summary_stats is a data.table\n}\nSpecifying scenarios\nThe possible values of each data generating parameter are specified as a vector. The function scenario_list creates all possible combinations of the values of the various parameters, so that there will be n1×n2×n3×…n_1 n_2 n_3 …n1×n2×n3×… scenarios, where nin_ini is the number of possible values for parameter iii. Examples of parameters might be sample size, effect size, variance, etc, really any value that can be used in the data generation process.\nThe process of data generation and model fitting is executed for each combination of n1×n2×n3×…n_1 n_2 n_3 …n1×n2×n3×… scenarios. This can be done locally using function lapply or using a high performance computing environment using something like Slurm_lapply in the slurmR package. (I won’t provide an example of that here - let me know if you’d like to see that.)\n#---- specify varying power-related parameters ---#\n\nscenario_list <- function(...) {\n  argmat <- expand.grid(...)\n  return(asplit(argmat, MARGIN = 1))\n}\n\nparam_1 <- c(...)\nparam_2 <- c(...)\nparam_3 <- c(...)\n.\n.\n.\n\nscenarios <- scenario_list(param1 = param_1, param_2 = param_2, param_3 = param_3, ...)\n\n#--- run locally ---#\n\nsummary_stats <- rbindlist(lapply(scenarios, function(a) s_replicate(a, nsim = 1000)))\nExample: power analysis of a CRT\nTo carry out a power analysis of a cluster randomized trial, I’ll fill in the skeletal framework. In this case I am interested in understanding how estimates of power vary based on changes in effect size, between cluster/site variation, and the number of patients per site. The data definitions use double dot notation to allow the definitions to change dynamically as we switch from one scenario to the next. We estimate a mixed effect model for each data set and keep track of the proportion of p-value estimates less than 0.05 for each scenario.\ns_define <- function() {\n  \n  #--- data definition code ---#\n  \n  def1 <- defData(varname = \"site_eff\", \n    formula = 0, variance = \"..svar\", dist = \"normal\", id = \"site\")\n  def1 <- defData(def1, \"npat\", formula = \"..npat\", dist = \"poisson\")\n  \n  def2 <- defDataAdd(varname = \"Y\", formula = \"5 + site_eff + ..delta * rx\", \n    variance = 3, dist = \"normal\")\n  \n  return(list(def1 = def1, def2 = def2)) \n}\n\ns_generate <- function(list_of_defs, argsvec) {\n  \n  list2env(list_of_defs, envir = environment())\n  list2env(as.list(argsvec), envir = environment())\n  \n  #--- data generation code ---#\n  \n  ds <- genData(40, def1)\n  ds <- trtAssign(ds, grpName = \"rx\")\n  dd <- genCluster(ds, \"site\", \"npat\", \"id\")\n  dd <- addColumns(def2, dd)\n  \n  return(dd)\n}\n\ns_model <- function(generated_data) {\n  \n  #--- model code ---#\n  \n  require(lme4)\n  require(lmerTest)\n  \n  lmefit <- lmer(Y ~ rx + (1|site), data = generated_data)\n  est <- summary(lmefit)$coef[2, \"Estimate\"]\n  pval <- summary(lmefit)$coef[2, \"Pr(>|t|)\"]\n  \n  return(data.table(est, pval)) # model_results is a data.table\n}\n\ns_single_rep <- function(list_of_defs, argsvec) {\n  \n  generated_data <- s_generate(list_of_defs, argsvec)\n  model_results <- s_model(generated_data)\n  \n  return(model_results)\n}\n\ns_replicate <- function(argsvec, nsim) {\n  \n  list_of_defs <- s_define()\n\n  model_results <- rbindlist(\n    parallel::mclapply(\n      X = 1 : nsim, \n      FUN = function(x) s_single_rep(list_of_defs, argsvec), \n      mc.cores = 4)\n  )\n  \n  #--- summary statistics ---#\n  \n  power <- model_results[, mean(pval <= 0.05)]\n  summary_stats <- data.table(t(argsvec), power)\n  \n  return(summary_stats) # summary_stats is a data.table\n}\nscenario_list <- function(...) {\n  argmat <- expand.grid(...)\n  return(asplit(argmat, MARGIN = 1))\n}\n\ndelta <- c(0.50, 0.75, 1.00)\nsvar <- c(0.25, 0.50)\nnpat <- c(8, 16)\n\nscenarios <- scenario_list(delta = delta, svar = svar, npat = npat)\n\n#--- run locally ---#\n\nsummary_stats <- rbindlist(lapply(scenarios, function(a) s_replicate(a, nsim = 250)))\nThe overall results (in this case, the power estimate) can be reported for each scenario.\nsummary_stats\n##     delta svar npat power\n##  1:  0.50 0.25    8 0.480\n##  2:  0.75 0.25    8 0.844\n##  3:  1.00 0.25    8 0.960\n##  4:  0.50 0.50    8 0.368\n##  5:  0.75 0.50    8 0.684\n##  6:  1.00 0.50    8 0.904\n##  7:  0.50 0.25   16 0.660\n##  8:  0.75 0.25   16 0.940\n##  9:  1.00 0.25   16 1.000\n## 10:  0.50 0.50   16 0.464\n## 11:  0.75 0.50   16 0.792\n## 12:  1.00 0.50   16 0.956\nWe can also plot the results easily to get a clearer picture. Higher between-site variation clearly reduces power, as do smaller effect sizes and smaller sizes. None of this is surprising, but is always nice to see things working out as expected:\nimg\n\n\n",
    "preview": {},
    "last_modified": "2021-03-25T09:40:07+11:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-03-24-how-to-treat-as-many-files-as-fit-on-your-hard-disk-without-loops-sorta-nor-running-out-of-memory-all-the-while-being-as-lazy-as-possible/",
    "title": "How to treat as many files as fit on your hard disk without loops (sorta) nor running out of memory all the while being as lazy as possible",
    "description": "Nifty demonstration of using quosures to load/read/process/dump files into/out of memory in R (case study processes 15k spreadsheets)",
    "author": [
      {
        "name": "Bruno Rodrigues",
        "url": "https://www.brodrigues.co/blog/2021-03-19-no_loops_tidyeval/"
      }
    ],
    "date": "2021-03-24",
    "categories": [],
    "contents": "\nIf you’ve been a faithful reader of this blog, or if you watch my youtube channel you’ve very likely seen me write code that looks like this:\nlibrary(tidyverse)\nlibrary(rlang)\nlibrary(tidyxl)\nlibrary(brotools)\nmtcars_plot <- mtcars %>%\n  group_nest(am) %>% #shortcut for group_by(am) %>% nest() \n  mutate(plots = map2(.y = am, .x = data, ~{ggplot(data = .x) +\n                              geom_smooth(aes(y = mpg, x = hp), colour = \"#82518c\") +\n                                ggtitle(paste0(\"Miles per gallon as a function of horse power for am = \", .y)) +\n                                theme_blog()}))\nThis creates a new data frame that looks like this:\nmtcars_plot\n## # A tibble: 2 x 3\n##      am           data plots \n##   <dbl> <list<tibble>> <list>\n## 1     0      [19 × 10] <gg>  \n## 2     1      [13 × 10] <gg>\nIn three lines of code, I grouped the mtcars dataframe by the variable am and then created two plots, which are contained in a new column called plots. If you’re unfamiliar with R, it is quite likely that you’ve never seen anything like this. If you have experience with functional programming languages though, you might recognize what’s going on. Essentially, map2() loops over two variables, am and data (this variable is not in the original data frame, but gets created as a result of the group_nest(am) call) and applies a function, in this case a call to ggplot(), to generate two plots… If you’ve never seen this before, I invite you to read the section dedicated to this type of workflows on my ebook.\nLet’s take a look at the plots:\nmtcars_plot %>%\n  pull(plots)\n## [[1]]\n## `geom_smooth()` using method = 'loess' and formula 'y ~ x'\nimg## \n## [[2]]\n## `geom_smooth()` using method = 'loess' and formula 'y ~ x'\nimgThe advantage of this workflow is that you don’t have to think much about anything -once you understand how it works-. The alternative would be two create two separate data frames, and create two separate plots. That’s a totally valid solution, unless you need to create hundreds of plots. With the workflow above, it doesn’t matter if the am variable has 2 or 2000 levels. The code would look exactly the same.\nThis workflow is very flexible. You can even use this approach to read in, and analyze, many, many files. As many as, for instance, 15931 Excel files from an American oil company that went bust in the early 2000’s, Enron.\nThe Enron Corpus\nI won’t go into much detail about the Enron Corpus, but to make a long story short: Big evil American oil company went bust, company emails got released for research purposes after being purchased for 10000USD by a computer scientist, and many of these emails had Excel spreadsheets attached to them. Other computer scientist released spreadsheets for research purposes. You can read the whole story on Felienne Hermans’ blog (read it, it’s quite interesting).\nAnyways, you can now get this treasure trove of nightmarish Excel spreadsheets by clicking here (this is the link provided in the blog post by Felienne Hermans). I already discussed this in a previous blog post.\nOn Felienne Hermans’ blog post, you can spot the following table:\nimgI’m going to show how this table could be replicated using R and the mutate()-map() workflow above.\nFirst, let’s load one single spreadsheet with {tidyxl} and get some of the code ready that we will need. Let’s get all the paths to all the files in a vector:\nlist_paths <- list.files(path = \"~/six_to/spreadsheets\",\n                         pattern = \".xlsx\",\n                         full.names = TRUE)\nLet’s work with the first one. Let’s read it in with {tidyxl}:\n(example_xlsx <- xlsx_cells(list_paths[1]))\n## # A tibble: 19,859 x 21\n##    sheet       address   row   col is_blank data_type error logical numeric\n##    <chr>       <chr>   <int> <int> <lgl>    <chr>     <chr> <lgl>     <dbl>\n##  1 Preschedule A1          1     1 FALSE    date      <NA>  NA           NA\n##  2 Preschedule B1          1     2 TRUE     blank     <NA>  NA           NA\n##  3 Preschedule C1          1     3 TRUE     blank     <NA>  NA           NA\n##  4 Preschedule D1          1     4 TRUE     blank     <NA>  NA           NA\n##  5 Preschedule E1          1     5 TRUE     blank     <NA>  NA           NA\n##  6 Preschedule F1          1     6 TRUE     blank     <NA>  NA           NA\n##  7 Preschedule G1          1     7 TRUE     blank     <NA>  NA           NA\n##  8 Preschedule H1          1     8 TRUE     blank     <NA>  NA           NA\n##  9 Preschedule I1          1     9 TRUE     blank     <NA>  NA           NA\n## 10 Preschedule J1          1    10 TRUE     blank     <NA>  NA           NA\n## # … with 19,849 more rows, and 12 more variables: date <dttm>, character <chr>,\n## #   character_formatted <list>, formula <chr>, is_array <lgl>,\n## #   formula_ref <chr>, formula_group <int>, comment <chr>, height <dbl>,\n## #   width <dbl>, style_format <chr>, local_format_id <int>\nThe beauty of {tidyxl} is that it can read in a very complex and ugly Excel file without any issues. Each cell of the spreadsheet is going to be one row of the data set, the contents of all cells is now easily accessible. Let’s see how many sheets are in there:\nexample_xlsx %>%\n  summarise(n_sheets = n_distinct(sheet))\n## # A tibble: 1 x 1\n##   n_sheets\n##      <int>\n## 1       11\n11… that’s already quite a lot. How many formulas are there per sheet?\nexample_xlsx %>%\n  mutate(is_formula = !is.na(formula)) %>%  \n  group_by(sheet) %>%\n  summarise(n_formula = sum(is_formula)) %>%\n  arrange(desc(n_formula))\n## # A tibble: 11 x 2\n##    sheet                  n_formula\n##    <chr>                      <int>\n##  1 Preschedule                 2651\n##  2 Deals                        324\n##  3 Economics                    192\n##  4 Balancing                     97\n##  5 Fuel                          70\n##  6 Comp                           0\n##  7 EPEData                        0\n##  8 HeatRate                       0\n##  9 spin reserve log sheet         0\n## 10 Top                            0\n## 11 Unit Summary                   0\nThere’s a sheet in there with 2651 formulas. This is insane. Anyways, as you can see, {tidyxl} makes analyzing what’s inside such Excel files quite simple. Let’s now create functions that will compute what we need. I won’t recreate everything from the table, but you’ll very quickly get the idea. Let’s start with a function to count spreadsheets that contain at least one formula:\nat_least_one_formula <- function(x){\n\n  (any(!is.na(x$formula)))\n\n}\nLet’s get the number of worksheets:\nn_sheets <- function(x){\n\n  x %>%\n    summarise(n_sheets =  n_distinct(sheet)) %>%\n    pull(n_sheets)\n\n}\nAnd how many formulas are contained in a spreadsheet:\nn_formulas <- function(x){\n\n  x %>%\n    mutate(is_formula = !is.na(formula)) %>%\n    summarise(n_formula = sum(is_formula)) %>%\n    pull(n_formula)\n\n}\nLet’s stop here. We could of course continue adding functions, but that’s enough to illustrate what’s coming. Let’s just define one last function. This function will call all three functions defined above, and return the result in a dataframe. You’ll see why soon enough:\nget_stats <- function(x){\n\n  tribble(~has_formula, ~n_sheets, ~n_formulas,\n          at_least_one_formula(x), n_sheets(x), n_formulas(x))\n\n}\nLet’s try it out on our single spreadsheet:\nget_stats(example_xlsx)\n## # A tibble: 1 x 3\n##   has_formula n_sheets n_formulas\n##   <lgl>          <int>      <int>\n## 1 TRUE              11       3334\nNeat.\nNow, let’s see how we can apply these function to 15k+ Excel spreadsheets.\nNo loops ever allowed\n10 years ago, I was confronted to a similar problem. I had a pretty huge amount of files on a computer that I needed to analyze for a chapter of my Phd thesis. The way I solved this issue was by writing a loop that looked horrible and did what I needed on each file. It did the job, but it did not look good, and was a nightmare whenever I needed to modify it, which I needed to do often. I had to think about a structure to hold the results; it was a nested list with I think 4 or 5 levels, and I had to keep track of the dimensions in my head to make sure I was writing the right result in the right spot. It wasn’t pleasant. Until this week, I thought that such a loop was the only real solution to such a problem.\nBut a comment on one of my youtube video changed this:\nClick to watch the Netflix adaptation of this blog postThe comment was made on this video in which I create a data set like in the introduction to this blog post, but instead of having 2 groups (and thus 2 datasets), I had 100. Now, in the video this wasn’t an issue, but what if instead of having 100 datasets, I had 15k+? And what if these datasets were quite huge? For example, the largest spreadsheet in the Enron Corpus is 40MiB. Loading it with {tidyxl} returns a tibble with 17 million rows, and needs 2GiB of RAM in a clean R session. If you want to read in all the 15k+, you’re simply going to run out of memory even before you could analyze anything. As I’ve written above, the solution would be to loop over each file, do whatever I need done, and save the results in some kind of structure (very likely some complex nested list). Or is it the only solution? Turns out that I tried some things out and found a solution that does not require changing my beloved mutate()-map() workflow.\nLet’s first start by putting the paths in a data frame:\n(enron <- enframe(list_paths, name = NULL, value = \"paths\"))\n## # A tibble: 15,871 x 1\n##    paths                                                                        \n##    <chr>                                                                        \n##  1 /home/cbrunos/six_to/spreadsheets/albert_meyers__1__1-25act.xlsx             \n##  2 /home/cbrunos/six_to/spreadsheets/albert_meyers__2__1-29act.xlsx             \n##  3 /home/cbrunos/six_to/spreadsheets/andrea_ring__10__ENRONGAS(1200).xlsx       \n##  4 /home/cbrunos/six_to/spreadsheets/andrea_ring__11__ENRONGAS(0101).xlsx       \n##  5 /home/cbrunos/six_to/spreadsheets/andrea_ring__12__ENRONGAS(1200).xlsx       \n##  6 /home/cbrunos/six_to/spreadsheets/andrea_ring__13__Trader & Products 5-15-01…\n##  7 /home/cbrunos/six_to/spreadsheets/andrea_ring__14__Trader & Products 5-16-01…\n##  8 /home/cbrunos/six_to/spreadsheets/andrea_ring__15__IFERCnov.xlsx             \n##  9 /home/cbrunos/six_to/spreadsheets/andrea_ring__16__ifercdec.xlsx             \n## 10 /home/cbrunos/six_to/spreadsheets/andrea_ring__17__IFERCJan.xlsx             \n## # … with 15,861 more rows\nFor the purposes of this blog post, let’s limit ourselves to 30 spreadsheets. This won’t impact how the code is going to work, nor memory usage. It’s just that I won’t my post to compile quickly while I’m writing:\n(enron <- head(enron, 30)) \n## # A tibble: 30 x 1\n##    paths                                                                        \n##    <chr>                                                                        \n##  1 /home/cbrunos/six_to/spreadsheets/albert_meyers__1__1-25act.xlsx             \n##  2 /home/cbrunos/six_to/spreadsheets/albert_meyers__2__1-29act.xlsx             \n##  3 /home/cbrunos/six_to/spreadsheets/andrea_ring__10__ENRONGAS(1200).xlsx       \n##  4 /home/cbrunos/six_to/spreadsheets/andrea_ring__11__ENRONGAS(0101).xlsx       \n##  5 /home/cbrunos/six_to/spreadsheets/andrea_ring__12__ENRONGAS(1200).xlsx       \n##  6 /home/cbrunos/six_to/spreadsheets/andrea_ring__13__Trader & Products 5-15-01…\n##  7 /home/cbrunos/six_to/spreadsheets/andrea_ring__14__Trader & Products 5-16-01…\n##  8 /home/cbrunos/six_to/spreadsheets/andrea_ring__15__IFERCnov.xlsx             \n##  9 /home/cbrunos/six_to/spreadsheets/andrea_ring__16__ifercdec.xlsx             \n## 10 /home/cbrunos/six_to/spreadsheets/andrea_ring__17__IFERCJan.xlsx             \n## # … with 20 more rows\nOk, so now, in order to read in all these files, I would write the following code:\nenron %>%\n  mutate(datasets = map(paths, xlsx_cells))\nThis would create a new column called datasets where each element would be a complete data set. If I run this in my 30 examples, it might be ok. But if I run it on the full thing, there’s no way I’m not going to run out of RAM. So how to solve this issue? How to run my neat get_stats() function on all datasets if I cannot read in the data? The solution is to only read in the data when I need it, and only one dataset at a time. The solution is to build a lazy tibble. And this is possible using quo(). To quickly grasp what quo() does, let’s try calling the following expression once with, and once without quo():\nrunif(10)\n##  [1] 0.98342755 0.13500737 0.06196822 0.61304269 0.30600919 0.48015570\n##  [7] 0.05747049 0.04535318 0.37880304 0.70647563\nThis runs runif(10) returning 10 randomly generated numbers, as expected.\nquo(unif(10))\n## <quosure>\n## expr: ^unif(10)\n## env:  global\nThis instead returns a quosure, which to be honest, is a complex beast. I’m not sure I get it myself. The definition, is that quosures are quoted expressions that keep track of an environment. For our practical purposes, we can use that to delay when the data gets read in, and that’s all that matters:\n(enron <- enron %>%\n   mutate(datasets = map(paths, ~quo(xlsx_cells(.)))))\n## # A tibble: 30 x 2\n##    paths                                                                datasets\n##    <chr>                                                                <list>  \n##  1 /home/cbrunos/six_to/spreadsheets/albert_meyers__1__1-25act.xlsx     <quosur…\n##  2 /home/cbrunos/six_to/spreadsheets/albert_meyers__2__1-29act.xlsx     <quosur…\n##  3 /home/cbrunos/six_to/spreadsheets/andrea_ring__10__ENRONGAS(1200).x… <quosur…\n##  4 /home/cbrunos/six_to/spreadsheets/andrea_ring__11__ENRONGAS(0101).x… <quosur…\n##  5 /home/cbrunos/six_to/spreadsheets/andrea_ring__12__ENRONGAS(1200).x… <quosur…\n##  6 /home/cbrunos/six_to/spreadsheets/andrea_ring__13__Trader & Product… <quosur…\n##  7 /home/cbrunos/six_to/spreadsheets/andrea_ring__14__Trader & Product… <quosur…\n##  8 /home/cbrunos/six_to/spreadsheets/andrea_ring__15__IFERCnov.xlsx     <quosur…\n##  9 /home/cbrunos/six_to/spreadsheets/andrea_ring__16__ifercdec.xlsx     <quosur…\n## 10 /home/cbrunos/six_to/spreadsheets/andrea_ring__17__IFERCJan.xlsx     <quosur…\n## # … with 20 more rows\nThis takes less than a second to run, and not just because I only have 30 paths. Even if I was working on the complete 15k+ datasets, this would run in an instant. That’s because we’re actually not reading in anything yet. We’re only setting the scene.\nThe magic happens now: we’re going to now map our function that computes the stats we need. We only need to change one thing. Let’s see:\nget_stats <- function(x){\n\n  x <- eval_tidy(x)\n\n  tribble(~has_formula, ~n_sheets, ~n_formulas,\n          at_least_one_formula(x), n_sheets(x), n_formulas(x))\n\n}\nI’ve added this line:\nx <- eval_tidy(x)\nThis evaluates the quosure, thus instantiating the dataset, and then proceeds to make all the computations. Let’s see what happens when we run this on our lazy tibble:\n(enron <- enron %>%\n   mutate(stats = map(datasets, get_stats)))\n## # A tibble: 30 x 3\n##    paths                                                  datasets  stats       \n##    <chr>                                                  <list>    <list>      \n##  1 /home/cbrunos/six_to/spreadsheets/albert_meyers__1__1… <quosure> <tibble [1 …\n##  2 /home/cbrunos/six_to/spreadsheets/albert_meyers__2__1… <quosure> <tibble [1 …\n##  3 /home/cbrunos/six_to/spreadsheets/andrea_ring__10__EN… <quosure> <tibble [1 …\n##  4 /home/cbrunos/six_to/spreadsheets/andrea_ring__11__EN… <quosure> <tibble [1 …\n##  5 /home/cbrunos/six_to/spreadsheets/andrea_ring__12__EN… <quosure> <tibble [1 …\n##  6 /home/cbrunos/six_to/spreadsheets/andrea_ring__13__Tr… <quosure> <tibble [1 …\n##  7 /home/cbrunos/six_to/spreadsheets/andrea_ring__14__Tr… <quosure> <tibble [1 …\n##  8 /home/cbrunos/six_to/spreadsheets/andrea_ring__15__IF… <quosure> <tibble [1 …\n##  9 /home/cbrunos/six_to/spreadsheets/andrea_ring__16__if… <quosure> <tibble [1 …\n## 10 /home/cbrunos/six_to/spreadsheets/andrea_ring__17__IF… <quosure> <tibble [1 …\n## # … with 20 more rows\nWhat happened here is nothing short of black magic: one by one, each quosure was instantiated, and the required stats were computed, then the dataset was thrown into the garbage before moving on to the next quosure. This means that RAM usage was kept to a minimum, and I could have run this over my 15k+ spreadsheets without any issue. You can watch me run similar code in my video here; I show how my RAM usage does not move even though I’m mapping over all the Excel sheets. The column stats now holds one dataframe with one row and three columns for each Excel file. Because stats is a list-column of dataframes, we can use unnest() to get to the data. Let’s take a closer look on one dataframe:\nenron %>%\n  head(1) %>%\n  select(paths, stats) %>%\n  unnest(cols = stats)\n## # A tibble: 1 x 4\n##   paths                                          has_formula n_sheets n_formulas\n##   <chr>                                          <lgl>          <int>      <int>\n## 1 /home/cbrunos/six_to/spreadsheets/albert_meye… TRUE              11       3334\nWe see that by using unnest(), the two columns inside the nested dataframe get expanded and become columns of the “main” dataframe.\nWe’re done, but let’s clean up the dataset a little bit and take a look at the results:\n(\n  enron <- enron %>%\n    mutate(excel_file = str_remove(paths, \"/home/cbrunos/six_to/spreadsheets/\")) %>%\n    select(-paths, -datasets) %>%\n    unnest(cols = stats)\n)\n## # A tibble: 30 x 4\n##    has_formula n_sheets n_formulas excel_file                                   \n##    <lgl>          <int>      <int> <chr>                                        \n##  1 TRUE              11       3334 albert_meyers__1__1-25act.xlsx               \n##  2 TRUE              11       3361 albert_meyers__2__1-29act.xlsx               \n##  3 TRUE               4        550 andrea_ring__10__ENRONGAS(1200).xlsx         \n##  4 TRUE               4        549 andrea_ring__11__ENRONGAS(0101).xlsx         \n##  5 TRUE               4        550 andrea_ring__12__ENRONGAS(1200).xlsx         \n##  6 FALSE              0          0 andrea_ring__13__Trader & Products 5-15-01 E…\n##  7 FALSE              0          0 andrea_ring__14__Trader & Products 5-16-01 E…\n##  8 TRUE               1        169 andrea_ring__15__IFERCnov.xlsx               \n##  9 TRUE               1        177 andrea_ring__16__ifercdec.xlsx               \n## 10 TRUE               1        162 andrea_ring__17__IFERCJan.xlsx               \n## # … with 20 more rows\nGetting some statistics is now easy:\nenron %>%\n  summarise(average_n_formulas = mean(n_formulas),\n            max_sheets = max(n_sheets))\n## # A tibble: 1 x 2\n##   average_n_formulas max_sheets\n##                <dbl>      <int>\n## 1               490.         11\nBy the way, now that we see that the code works, we can run it on all the spreadsheets simply by not running the following line:\n(enron <- head(enron, 30)) \nAlso, we can quite easily run all of this in parallel using {furrr}:\nlibrary(furrr)\n## Loading required package: future\nplan(multiprocess, workers = 12)\n\nenron <- enframe(list_paths, name = NULL, value = \"paths\")\n\nenron <- head(enron, 1200) #just to compile the document faster, I only consider 1200 Excel spreadsheets\n\nenron <- enron %>%\n   mutate(datasets = map(paths, ~quo(xlsx_cells(.))))\n\nstart <- Sys.time()\nenron <- enron %>%\n  mutate(stats = future_map(datasets, get_stats))\nSys.time() - start\n## Time difference of 36.86839 secs\nSame code, no parallelization (it takes longer, obviously):\nenron <- enframe(list_paths, name = NULL, value = \"paths\")\n\nenron <- head(enron, 1200)\n\nenron <- enron %>%\n   mutate(datasets = map(paths, ~quo(xlsx_cells(.))))\n\nstart <- Sys.time()\nenron <- enron %>%\n  mutate(stats = map(datasets, get_stats))\nSys.time() - start\n## Time difference of 1.217199 mins\nI think this is pretty neat.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-03-24T10:17:58+11:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-03-24-simplifying-geospatial-features-in-r-with-sf-and-rmapshaper/",
    "title": "Simplifying geospatial features in R with sf and rmapshaper",
    "description": "Cool overview of how to simplify geospatial features to save memory",
    "author": [
      {
        "name": "Markus Konrad",
        "url": "https://datascience.blog.wzb.eu/2021/03/15/simplifying-geospatial-features-in-r-with-sf-and-rmapshaper/"
      }
    ],
    "date": "2021-03-24",
    "categories": [],
    "contents": "\nWhen working with geospatial data, memory consumption and computation time can become quite a problem, since these datasets are often very large. You may have very granular, high resolution data although that isn’t really necessary for your use-case, for example when plotting large scale maps or when applying calculations at a spatial level for which lower granularity is sufficient. In such scenarios, it’s best to first simplify the geospatial features (the sets of points, lines and polygons that represent geographic entities) in your data. By simplify I mean that we generally reduce the amount of information that is used to represent these features, i.e. we remove complete features (e.g. small islands), we join features (e.g. we combine several overlapping or adjacent features) or we reduce the complexity of features (e.g. we remove vertices or holes in a feature). Since applying these operations comes with information loss you should be very careful about how much you simplify and if this in some way biases you results.\nIn R, we can apply this sort of simplification with a few functions from the packages sf and, for some special cases explained below, rmapshaper. In the following, I will show how to apply them and what the effects of their parameters are. The data and source code are available on GitHub.\nFirst, let’s load the required libraries. Besides the already mentioned sf package, we also load ggplot2 for visualization and magrittr, because I will use the %>% pipe operator sometimes.\nlibrary(ggplot2)\nlibrary(magrittr)\nlibrary(sf)\nI will also use a utility function plot_map() which is capable of plotting geospatial features, zooming into certain spots and highlighting differences between two features. The lengthy source code for this function is available in the Rmarkdown document of the related GitHub repository.\nLoading and transforming our sample data\nLet’s load our first example dataset. It represents the German federal state Mecklenburg-Vorpommern (or Mecklenburg-West Pomerania) that features a ragged coastline at the Baltic Sea, which is nice to show the effects of feature simplification. I will abbreviate Mecklenburg-Vorpommern by MV to spare us the complicated name.\nmv <- st_read('data/mv.geojson') %>%\n  st_geometry() %>%\n  st_transform(crs = '+proj=aeqd +lat_0=53.6 +lon_0=12.7')\n    # centered at input data for low distortion\nThe dataset contains only a single feature (a multi-polygon, i.e. a set of polygons) with some metadata from OpenStreetMap. I use st_geometry to access this feature (i.e. dismiss the metadata) and st_transform to transform it to an Azimuthal Equidistant map projection. The latter is quite important. Most functions that I’ll use don’t work with spherical coordinates as used for example by the WGS84 (GPS) standard, where longitude and latitude in degrees describe a position on a sphere. We need to project these coordinates on a plane using a coordinate reference system (CRS) with a specific projection. All projections distort in some way, but we can choose a CRS that accurately represents certain measures. For example, I chose an equidistant CRS because it accurately represents distances up to 10,000 km from the center of projection. Another important aspect is that by using this CRS, we move from degrees to meters as units for our coordinates.\nLet’s plot our projection of MV. Note the axes which represent the distance from the projection center (I chose the geographic center of MV for that) in meters.\nplot_map(mv, graticules = TRUE)\nimgLet’s zoom in on the island of Rügen with it’s ragged coastline (west of it is the peninsula Darß, east is a part of Usedom).\nplot_map(mv, graticules = TRUE, zoom_to = c(13.359, 54.413), \n         zoom_level = 8)\nimgRemoving vertices with st_simplify\nWe can now continue to apply different simplification methods to the displayed features. We will start with – no surprise – st_simplify. This function removes vertices in lines or polygons to form simpler shapes. The function implementation uses the Douglas–Peucker algorithm for simplification and accepts a parameter dTolerance, which roughly speaking specifies the distance in which any “wiggles” will be straightened. It’s in the same unit as the input data, so in our case it’s meters (the unit used in our CRS). There’s also a parameter preserveTopology which, when set to TRUE, makes sure that polygons are not reduced to lines or even removed, or that inner holes in them are removed during the simplification process.\nLet’s apply st_simplify with a tolerance of 1km:\nmv_simpl <- st_simplify(mv, preserveTopology = FALSE, dTolerance = 1000)\nplot_map(mv_simpl)\nimgWe can see that the geometry is much simpler, especially when zooming in:\nplot_map(mv_simpl, zoom_to = c(13.359, 54.413), zoom_level = 8)\nimgWe can also confirm the memory usage is much lower for the simplified feature (shown in kilobytes):\nround(c(object.size(mv), object.size(mv_simpl)) / 1024)\n[1] 960  13\nThe purple areas show the differences between the original and the simplified version. We can see for example, that some smaller islands were completely removed.\nplot_map(mv, mv_simpl, zoom_to = c(13.359, 54.413),\n         zoom_level = 8)\nimgWhen we set preserveTopology to TRUE, we can observe that the small islands are retained:\nmv_simpl <- st_simplify(mv, preserveTopology = TRUE,\n                        dTolerance = 1000)\nplot_map(mv_simpl, zoom_to = c(13.359, 54.413), zoom_level = 8)\nimgIncreasing the dTolerance parameter to 10km gives us something that is more akin to a Cubistic painting:\nmv_simpl <- st_simplify(mv, preserveTopology = FALSE, \n                        dTolerance = 10000)\nplot_map(mv_simpl)\nimgplot_map(mv, mv_simpl)\nimgst_simplify limitations\nUsing st_simplify comes with a limitation when working with multiple, adjacent features which I’d like to demonstrate using a map of the federal states in Germany.\nfedstates <- read_sf('data/germany_fedstates.geojson') %>%\n  st_transform(crs = 5243)  # ETRS89 / LCC Germany (E-N)\nfedstates[c('name', 'geometry')]\n## Simple feature collection with 16 features and 1 field\n## geometry type:  MULTIPOLYGON\n…\n## projected CRS:  ETRS89 / LCC Germany (E-N)\n## # A tibble: 16 x 2\n…\n##  1 Baden-Württemberg (((-112269.7 -363339.8, …\n##  2 Bavaria           (((-109036.3 -104479.2, …\n##  3 Berlin            (((175933 160909.2, …\n…\nThis time, our spatial dataset contains 16 features which represent the boundaries of each federal state:\nplot_map(fedstates, graticules = TRUE, strokecolor = '#097FB3',\n         fillcolor = '#AED3E4')\nimgLet’s apply st_simplify with a tolerance of 10km and preserving feature topology:\nfedstates_simpl <- st_simplify(fedstates,\n                               preserveTopology = TRUE, \n                               dTolerance = 10000)\nplot_map(fedstates_simpl, strokecolor = '#097FB3',\n         fillcolor = '#AED3E4')\nimgUps! We can see that st_simplify produced gaps and overlapping features, i.e. shared borders were not handled correctly! The problem is that st_simplify simply doesn’t consider the topological concept of shared borders between features (like federal states in this case). When setting preserveTopology = TRUE it means that each feature’s topology is preserved, but it doesn’t mean that the topology between features is considered.\nLuckily, there’s the package rmapshaper that contains a function that does the trick. Before you install the package, please note that it requires a lot of (system-) dependencies to be installed.\nVisvalingam’s algorithm, which is used in this function, let’s us specify the portion of vertices that should be kept after simplification. This can be specified with the keep parameter. The parameter keep_shapes controls whether complete features such as islands can be removed in the simplification process.\nlibrary(rmapshaper)\n\nfedstates_simpl2 <- ms_simplify(fedstates, keep = 0.001,\n                                keep_shapes = FALSE)\nplot_map(fedstates_simpl2, strokecolor = '#097FB3',\n         fillcolor = '#AED3E4')\nimgAs can be seen, ms_simplify respects the shared borders. The result is also much smaller in terms of memory usage:\nround(c(object.size(fedstates),\n        object.size(fedstates_simpl2)) / 1024)\n[1] 7757   71\nExpanding and shrinking with st_buffer\nst_buffer lets you expand (positive buffer distance) or shrink (negative buffer distance) polygon features. This may not per-se result in a simpler feature form, but it allows you to close holes (by expanding) or remove solitary, small features (by shrinking).\nLet’s try a first example. Again, the buffer distance is in the same unit as our CRS, which is meters in our case, and we’ll expand by 1km. When the buffer is generated, new vertices are added to form the expanded shape with round edges. The parameter nQuadSegs specifies how many segments are generated per quadrant and feature.\nmv_buf <- st_buffer(mv, dist = 1000, nQuadSegs = 1)\nplot_map(mv_buf, graticules = TRUE)\nimgplot_map(mv, mv_buf, graticules = TRUE,\n         zoom_to = c(13.359, 54.413), zoom_level = 8)\nimgThe memory usage is not so much reduced:\nround(c(object.size(mv), object.size(mv_buf)) / 1024)\n[1] 960 265\nNow we shrink MV by 1km:\nmv_buf <- st_buffer(mv, -1000, nQuadSegs = 1)\nplot_map(mv_buf)\nimgplot_map(mv, mv_buf, zoom_to = c(13.359, 54.413),\n         zoom_level = 8)\nimgWe can also combine both buffering options, by first shrinking in order to remove small isolated features and then expanding again by the same distance so that the result has more or less the same extents as the original:\nmv_buf <- st_buffer(mv, -1000, nQuadSegs = 1) %>%\n  st_buffer(1000, nQuadSegs = 1)\nplot_map(mv_buf)\nimgplot_map(mv, mv_buf, zoom_to = c(13.359, 54.413),\n         zoom_level = 8)\nimgNote how this is more aggressive than st_simplify but the result uses more memory. We could now additionally apply st_simplify to arrive at a very reduced feature.\nround(c(object.size(mv), object.size(mv_buf)) / 1024)\n[1] 960 239\nWhen we apply st_buffer to a dataset with multiple features like the federal states dataset, the buffering will be applied to every feature (i.e. every state’s boundary) separately. This will result in either overlapping features (extending) or gaps between them (shrinking).\nJoining overlapping and adjacent features with st_union\nThe last function that we will look at is st_union. It can be used to join overlapping and adjacent features. With this, we could for example join all features of the federal state boundaries in Germany and get a single feature that represents the national boundaries. But let’s look at a more interesting example and load a new dataset. It contains a sample with regions with street noise of 50dB or more in Berlin at night.\nnoise <- read_sf('data/noise_berlin_sample.geojson') %>%\n  st_transform('+proj=aeqd +lat_0=52.51883 +lon_0=13.41537')\n\nplot_map(noise, graticules = TRUE)\nimgYou may notice the ragged edges in the picture above. Furthermore, the dataset is quite large and contains a lot of rows (i.e. features):\nnrow(noise)\n[1] 48795\nWhy is that so? When we zoom in and also plot each feature’s boundary, we will notice that we actually have some sort of vectorized raster data! The noise regions are tiny raster cells:\nplot_map(noise, strokecolor = '#097FB3', fillcolor = '#AED3E4',\n         zoom_level = 15)\nimgMost cells cover 100m² and come along with the noise level at this spot (L_N column):\nsummary(noise[c('Shape_Area', 'L_N')])\n  Shape_Area         L_N                 geometry    \n  Min.   :100.0   Min.   :50.00   MULTIPOLYGON :48795  \n  1st Qu.:100.0   1st Qu.:54.10   epsg:NA      :    0  \n  Median :100.0   Median :58.50   +proj=aeqd…:    0  \n  Mean   :105.1   Mean   :59.12                        \n  3rd Qu.:100.0   3rd Qu.:63.80                        \n  Max.   :700.0   Max.   :75.40  \nIf we don’t care for the specific noise levels and only want to form a single feature that represents noise levels with 50dB or more, we can apply st_union on the whole dataset:\nnoise_union <- st_union(noise)\nnoise_union\nGeometry set for 1 feature \n geometry type:  MULTIPOLYGON\n dimension:      XY\n bbox:           xmin: -2218.35 ymin: -2035.134\n                 xmax: 2174.847 ymax: 845.1362\n CRS:            +proj=aeqd +lat_0=52.51883 +lon_0=13.41537\n MULTIPOLYGON (((-1931.274 -2034.915, -1941.274 …\nAs we can see in the output above, only a single feature is left (and all feature-specific “metadata” such as level of noise is gone!). We can confirm this visually:\nplot_map(noise_union, strokecolor = '#097FB3',\n         fillcolor = '#AED3E4', zoom_level = 15)\nimgplot_map(noise_union, strokecolor = '#097FB3',\n         fillcolor = '#AED3E4')\nimgAdditionally applying st_simplify removes the ragged edges:\nnoise_union_simpl <- st_simplify(noise_union,\n                                 preserveTopology = FALSE, \n                                 dTolerance = 50)\nplot_map(noise_union_simpl, strokecolor = '#097FB3',\n         fillcolor = '#AED3E4')\nimgNote how much the memory consumption was reduced from initially 42 megabytes to 282 kilobytes or even 18 kilobytes when further simplified:\nround(c(object.size(noise), object.size(noise_union),\n        object.size(noise_union_simpl)) / 1024)\n[1] 41959   282    18\nApplying simplify without union before would not have had the desired effect since st_simplify is performed per feature:\nst_simplify(noise, preserveTopology = FALSE,\n            dTolerance = 10) %>%\n  plot_map(strokecolor = '#097FB3', fillcolor = '#AED3E4')\nimgIf we want to retain some of the noise level information, we can quantize (“bin”) the noise level data into different ordered categories and apply the union operation per category. I’m used to the dplyr package, so I’ll first create a quantized version of the noise variable L_N with three equally spaced levels between 50dB and 80dB and then use group_by in conjunction with group_modify:\nlibrary(dplyr)\n\nnoise_per_lvl <-\n  mutate(noise,\n         level = cut(L_N,\n                     breaks = seq(50, 80, by = 10),\n                     right = FALSE,\n                     ordered_result = TRUE)) %>%\n  group_by(level) %>%  # \".x\" is refers to the current group:\n  group_modify(~ st_union(.x) %>% as_tibble()) %>%\n  ungroup() %>%\n  st_as_sf()      # convert back to \"sf\" object since this \n                  # information is lost during group_by()\nnoise_per_lvl\n## Simple feature collection with 3 features and 1 field\n## geometry type:  MULTIPOLYGON\n## dimension:      XY\n## bbox:           xmin: -2218.35 ymin: -2035.134…\n## CRS:            +proj=aeqd +lat_0=52.51883… \n## # A tibble: 3 x 2\n##   level   geometry\n## 1 [50,60) (((1908.792 -1950.634, 1898.792 -1950.854…\n## 2 [60,70) (((1758.789 -1953.926, 1748.789 -1954.146…\n## 3 [70,80) (((-527.0325 -373.2836, -537.0327 -373.50…\nAgain, the result is much smaller in terms of memory:\nround(c(object.size(noise), object.size(noise_per_lvl)) / 1024)\n[1] 41959  1090\nThe following gives us a plot with the three noise levels:\nggplot(noise_per_lvl) +\n  geom_sf(aes(color = level, fill = level)) +\n  coord_sf(datum = NA) +\n  theme_bw()\nimgConclusion\nAll in all, it’s wise to first think about simplifying your spatial data before you continue to work with it. Spatial datasets are often very large and depending on your use-case you may not need a high level of detail. It may even be absolutely necessary to reduce the complexity of your data, e.g. when the computational capacities are limited or when you produce interactive maps for the web. As shown, R has all the tools that you need to simplify spatial data with the packages sf and rmapshaper.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-03-24T10:04:30+11:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-03-11-clustering-similar-spatial-patterns/",
    "title": "Clustering similar spatial patterns",
    "description": "A short description of the post.",
    "author": [
      {
        "name": "Jakub Nowosad",
        "url": "https://nowosad.github.io/post/motif-bp5/"
      }
    ],
    "date": "2021-03-11",
    "categories": [],
    "contents": "\nTLTR:  Clustering similar spatial patterns requires one or more raster datasets for the same area. Input data is divided into many sub-areas, and spatial signatures are derived for each sub-area. Next, distances between signatures for each sub-area are calculated and stored in a distance matrix. The distance matrix can be used to create clusters of similar spatial patterns. Quality of clusters can be assessed visually using a pattern mosaic or with dedicated quality metrics.\nSpatial data\nTo reproduce the calculations in the following post, you need to download all of relevant datasets using the code below:\nlibrary(osfr)\ndir.create(\"data\")\nosf_retrieve_node(\"xykzv\") %>%\n        osf_ls_files(n_max = Inf) %>%\n        osf_download(path = \"data\",\n                     conflicts = \"overwrite\")\nYou should also attach the following packages:\nlibrary(sf)\nlibrary(stars)\nlibrary(motif)\nlibrary(tmap)\nlibrary(dplyr)\nlibrary(readr)\nLand cover and landforms in Africa\nThe data/land_cover.tif contains land cover data and data/landform.tif is landform data for Africa. Both are single categorical rasters of the same extent and the same resolution (300 meters) that can be read into R using the read_stars() function.\nlc = read_stars(\"data/land_cover.tif\")\nlf = read_stars(\"data/landform.tif\")\nAdditionally, the data/lc_palette.csv file contains information about colors and labels of each land cover category, and data/lf_palette.csv stores information about colors and labels of each landform class.\nlc_palette_df = read_csv(\"data/lc_palette.csv\")\nlf_palette_df = read_csv(\"data/lf_palette.csv\")\nnames(lc_palette_df$color) = lc_palette_df$value\nnames(lf_palette_df$color) = lf_palette_df$value\nBoth datasets can be visualized with tmap.\ntm_lc = tm_shape(lc) +\n        tm_raster(style = \"cat\",\n                  palette = lc_palette_df$color,\n                  labels = lc_palette_df$label,\n                  title = \"Land cover:\") +\n        tm_layout(legend.position = c(\"LEFT\", \"BOTTOM\"))\ntm_lc\nimgtm_lf = tm_shape(lf) +\n        tm_raster(style = \"cat\",\n                  palette = lf_palette_df$color,\n                  labels = lf_palette_df$label,\n                  title = \"Landform:\") +\n        tm_layout(legend.outside = TRUE)\ntm_lf\nimgWe can combine these two datasets together with the c() function.\neco_data = c(lc, lf)\nThe problem now is how to find clusters of similar spatial patterns of both land cover categories and landform classes.\nClustering spatial patterns\nThe basic step in clustering spatial patterns is to calculate a proper signature for each spatial window using the lsp_signature() function. Here, we use the integrated co-occurrence vector (type = \"cove\") representation. In this example, we use a window of 300 cells by 300 cells (window = 300). This means that our search scale will be 90 km (300 cells x data resolution) - resulting in dividing the whole area into about 7,500 regular rectangles of 90 by 90 kilometers.\nThis operation could take a few minutes.\neco_signature = lsp_signature(eco_data,\n                              type = \"incove\",\n                              window = 300)\nThe output, eco_signature contains numerical representation for each 90 by 90 km area. Notice that it has 3,838 rows (not 7,500) - this is due to removing areas with a large number of missing values before calculations1.\nDistance matrix\nNext, we can calculate the distance (dissimilarity) between patterns of each area. This can be done with the lsp_to_dist() function, where we must provide the output of lsp_signature() and a distance measure used (dist_fun = \"jensen-shannon\"). This operation also could take a few minutes.\neco_dist = lsp_to_dist(eco_signature, dist_fun = \"jensen-shannon\")\nThe output, eco_dist, is of a dist class, where small values show that two areas have a similar joint spatial pattern of land cover categories and landform classes.\nclass(eco_dist)\n\n## [1] \"dist\"\nHierarchical clustering\nObjects of class dist can be used by many existing R functions for clustering. It includes different approaches of hierarchical clustering (hclust(), cluster::agnes(), cluster::diana()) or fuzzy clustering (cluster::fanny()). In the below example, we use hierarchical clustering using hclust(), which expects a distance matrix as the first argument and a linkage method as the second one. Here, we use the Ward’s minimum variance method (method = \"ward.D2\") that minimizes the total within-cluster variance.\neco_hclust = hclust(eco_dist, method = \"ward.D2\")\nplot(eco_hclust)\nimgGraphical representation of the hierarchical clustering is called a dendrogram, and based on the obtained dendrogram, we can divide our local landscapes into a specified number of groups using cutree(). In this example, we use eight classes (k = 8) to create a fairly small number of clusters to showcase the presented methodology.\nclusters = cutree(eco_hclust, k = 8)\nHowever, a decision about the number of clusters in real-life cases should be based on the goal of the research.\nClustering results\nThe lsp_add_clusters function adds: a column clust with a cluster number to each area, and converts the result to an sf object.\neco_grid_sf = lsp_add_clusters(eco_signature,\n                               clusters)\nThe clustering results can be further visualized using tmap.\ntm_clu = tm_shape(eco_grid_sf) +\n        tm_polygons(\"clust\", style = \"cat\", palette = \"Set2\", title = \"Cluster:\") +\n        tm_layout(legend.position = c(\"LEFT\", \"BOTTOM\"))\ntm_clu\nimgMost clusters form continuous regions, so we could merge areas of the same clusters into larger polygons.\neco_grid_sf2 = eco_grid_sf %>%\n        dplyr::group_by(clust) %>%\n        dplyr::summarize()\nThe output polygons can then be superimposed on maps of land cover categories and landform classes.\ntm_shape(eco_data) +\n                tm_raster(style = \"cat\",\n                          palette = list(lc_palette_df$color, lf_palette_df$color)) +\n  tm_facets(ncol = 2) +\n  tm_shape(eco_grid_sf2) +\n  tm_borders(col = \"black\") +\n  tm_layout(legend.show = FALSE, \n            title.position = c(\"LEFT\", \"TOP\"))\nimgWe can see that many borders (black lines) contain areas with both land cover or landform patterns distinct from their neighbors. Some clusters are also only distinct for one variable (e.g., look at Sahara on the land cover map).\nClustering quality\nWe can also calculate the quality of the clusters with the lsp_add_quality() function. It requires an output of lsp_add_clusters() and an output of lsp_to_dist(), and adds three new variables: inhomogeneity, distinction, and quality.\neco_grid_sfq = lsp_add_quality(eco_grid_sf, eco_dist, type = \"cluster\")\nInhomogeneity (inhomogeneity) measures a degree of mutual distance between all objects in a cluster. This value is between 0 and 1, where the small value indicates that all objects in the cluster represent consistent patterns, so the cluster is pattern-homogeneous. Distinction (distinction) is an average distance between the focus cluster and all the other clusters. This value is between 0 and 1, where the large value indicates that the cluster stands out from the rest of the clusters. Overall quality (quality) is calculated as 1 - (inhomogeneity / distinction). This value is also between 0 and 1, where increased values indicate better quality of clustering.\nWe can create a summary of each clusters’ quality using the code below.\neco_grid_sfq2 = eco_grid_sfq %>%\n        group_by(clust) %>%\n        summarise(inhomogeneity = mean(inhomogeneity),\n                  distinction = mean(distinction),\n                  quality = mean(quality))\nclust\ninhomogeneity\ndistinction\nquality\n1\n0.5064706\n0.7724361\n0.3443204\n2\n0.4038704\n0.7023297\n0.4249561\n3\n0.3377875\n0.7065250\n0.5219029\n4\n0.1161293\n0.7921515\n0.8534002\n5\n0.3043422\n0.7366735\n0.5868696\n6\n0.2774136\n0.6849140\n0.5949657\n7\n0.2926504\n0.7149212\n0.5906537\n8\n0.3486704\n0.7579511\n0.5399830\nThe created clusters show a different degree of quality metrics. The fourth cluster has the lowest inhomogeneity and the largest distinction, and therefore the best quality. The first cluster has the most inhomogeneous patterns, and while its distinction from other clusters is relatively large, its overall quality is the worst.\ntm_inh = tm_shape(eco_grid_sfq2) +\n        tm_polygons(\"inhomogeneity\", style = \"cont\", palette = \"magma\")\n\ntm_iso = tm_shape(eco_grid_sfq2) +\n        tm_polygons(\"distinction\", style = \"cont\", palette = \"-inferno\")\n\ntm_qua = tm_shape(eco_grid_sfq2) +\n        tm_polygons(\"quality\", style = \"cont\", palette = \"Greens\")\n\ntm_cluster3 = tmap_arrange(tm_clu, tm_qua, tm_inh, tm_iso, ncol = 2)\ntm_cluster3\nimgUnderstanding clusters\nInhomogeneity can also be assessed visually with a pattern mosaic. Pattern mosaic is an artificial rearrangement of a subset of randomly selected areas belonging to a given cluster.\nUsing the code below, we randomly selected 100 areas for each cluster. It could take a few minutes.\neco_grid_sample = eco_grid_sf %>% \n  filter(na_prop == 0) %>% \n  group_by(clust) %>% \n  slice_sample(n = 100)\nNext, we can extract a raster for each selected area with the lsp_add_examples() function.\neco_grid_examples = lsp_add_examples(eco_grid_sample, eco_data)\nFinally, we can use the lsp_mosaic() function, which creates raster mosaics by rearranging spatial data for sample areas. Note that this function is still experimental and can change in the future.\neco_mosaic = lsp_mosaic(eco_grid_examples)\nThe output is a stars object with the third dimension (clust) representing clusters, from which we can use slice() to extract a raster mosaic for a selected cluster. For example, the raster mosaic for fourth cluster looks like this:\neco_mosaic_c4 = slice(eco_mosaic, clust, 4)\n\ntm_shape(eco_mosaic_c4) +\n  tm_raster(style = \"cat\",\n            palette = list(lc_palette_df$color, lf_palette_df$color)) +\n  tm_facets(ncol = 2) +\n  tm_layout(legend.show = FALSE)\nimgWe can see that the land cover patterns for this cluster are very simple and homogeneous. The landform patterns are slightly more complex and less homogeneous.\nAnd the raster mosaic for first cluster is:\neco_mosaic_c1 = slice(eco_mosaic, clust, 1)\n\ntm_shape(eco_mosaic_c1) +\n  tm_raster(style = \"cat\",\n            palette = list(lc_palette_df$color, lf_palette_df$color)) +\n  tm_facets(ncol = 2) +\n  tm_layout(legend.show = FALSE)\nimgPatterns of both variables in this cluster are more complex and heterogeneous. This result could suggest that additional clusters could be necessary to distinguish some spatial patterns.\nSummary\nThe pattern-based clustering allows for grouping areas with similar spatial patterns. The above example shows the search based on two-variable raster data (land cover and landform), but by using a different spatial signature, it can be performed on a single variable raster as well. R code for the pattern-based clustering can be found here, with other examples described in the Spatial patterns’ clustering vignette.\nSee the threshold argument for more details. ↩︎\n« Previous page: Quantifing changes of spatial patterns\nNext page: Considerations for the pattern-based spatial analysis »\n\n\n\n",
    "preview": {},
    "last_modified": "2021-03-11T09:48:48+11:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-03-11-exploring-wednesday-night-cable-ratings-with-ocr/",
    "title": "Exploring Wednesday Night Cable Ratings with OCR",
    "description": "Interesting blog post where tabular data is extracted from IMAGES of the tables",
    "author": [
      {
        "name": "jtlawren67",
        "url": "https://jlaw.netlify.app/2021/03/01/exploring-wednesday-night-cable-ratings-with-ocr/"
      }
    ],
    "date": "2021-03-11",
    "categories": [],
    "contents": "\nOne of my guilty pleasure TV shows is MTV’s The Challenge. Debuting in the late 90s, the show pitted alumni from The Real World and Road Rules against each other in a series of physical events. Now on its 36th season, its found new popularity by importing challengers from other Reality Shows, in the US and Internationally, regularly topping Wednesday Night ratings in the coveted 18-49 demographic.\nLooking at the Ratings on showbuzzdaily.com shows that the Challenge was in fact #1 in this demographic. However, it also scores incredibly low on the 50+ demo.\nimgSo I figured that exploring the age and gender distributions of Wednesday Night Cable ratings would be interesting. The only caveat is… the data exists in an image.\nSo for this blog post, I will be extracting the ratings data from the image and doing some exploration on popular shows by age and gender.\nAlso, huge thanks to Thomas Mock and his The Mockup Blog for serving as a starting point for learning magick.\nUsing magick to process image data\nI’ll be using the magick package to read in the image and do some processing to clean up the image. Then I will use the ocr() function from the tesseract package to actual handle extraction of the data from the image.\nlibrary(tidyverse) #Data Manipulation\nlibrary(magick) #Image Manipulation\nlibrary(tesseract) #Extracting Text from the Image\nlibrary(patchwork) #Combining Multiple GGPLOTs Together\nThe first step is reading in the raw image from the showbuzzdaily.com website which can be done through magick’s image_read() function.\nraw_img <- image_read(\"http://www.showbuzzdaily.com/wp-content/uploads/2021/02/Final-Cable-2021-Feb-03-WED.png\")\n\nimage_ggplot(raw_img)\nimgThe next thing to notice is that while most of the data does exist in a tabular format, there are also headers and footers that don’t follow the tabular structure. So I’ll use image_crop() to keep only the tabular part of the image. The crop function uses a geometry_area() helper function which takes in four parameters. I struggled a bit with the documentation figuring out exactly how to get this working right but eventually internalized geometry_area(703, 1009, 0, 91) as “crop out 703 pixels of width and 1009 pixels of height starting from X-position on the left boundary and y-position 91 pixels from the top”.\nchopped_image <- \n  raw_img %>% \n  #crop out width:703px and height:1009px starting +91px from the top\n  image_crop(geometry_area(703, 1009, 0, 91)) \n\nimage_ggplot(chopped_image)\nimgNow the non-tabular data (header and footer) have been removed.\nThe ocr() algorithm that will handle extracting the data from the image can struggle with parts of the image as is. For example, it might think the color boundary between white and green is a character. Therefore, I’m going to try to do the best I can do clean up the image so that the ocr() function can have an easier time. Ultimately this required a lot of guess and check but in the end, I only did two steps for cleaning:\nRunning a morphology method over the image to remove the horizontal lines separating each group of 5 shows (this required negating the colors of the image so that the filter would have an easier time since white is considered foreground by default). The morphology method modifies an image based on the neighborhood of pixels around it and thinning is subtracting pixels from a shape. So by negating the color the method turns “non-black” pixels to black. Then re-negating turns everything back to “white”.\nTurning everything to greyscale to remove remaining colors.\nI had tried to remove the color gradients, but it took much more effort and was ultimately not more effective than just going to greyscale.\nprocessed_image <- chopped_image %>% \n  image_negate() %>% #Flip the Colors\n  # Remove the Horizontal Lines\n  image_morphology(method = \"Thinning\", kernel = \"Rectangle:7x1\") %>% \n  # Flip the Colors back to the original\n  image_negate() %>% \n  # Turn colors to greyscale\n  image_quantize(colorspace = \"gray\")\n\n\nimage_ggplot(processed_image)\nimgExtracting the Data with OCR\nBecause I can be lazy, my first attempts at extraction was just to run ocr() on the processed image and hope for the best. However, the best was somewhat frustrating. For example,\nocr(processed_image) %>% \n  str_sub(end = str_locate(., '\\\\n')[1])\n## [1] \"1 CHALLENGE: DOUBLE AGENMTV e:00PM 90/0.54 069 0.39 |047 053 0.20 |058 013} 920\\n\"\nJust looking at the top row there are a number of issues that come from just using ocr() directly on the table. The boundary between sections are showing up as “|” or “/” and sometime the decimal doesn’t appear.\nFortunately the function allows you to “whitelist” characters in order to nudge the algorithm on what it should expect to see. So rather than guess and check on the processing of the image to make everything work perfectly. I’ll write a function that allows me to crop to individual columns and specify the proper whitelist for each column.\nocr_text <- function(col_width, col_start, format_code){\n  \n  ##For Stations Which Are Only Characters\n  only_chars <- tesseract::tesseract(\n    options = list(\n      tessedit_char_whitelist = paste0(LETTERS, collapse = '')\n    )\n  )\n  \n  #For Titles Which Are Letters + Numbers + Characters\n  all_chars <- tesseract::tesseract(\n    options = list(\n      tessedit_char_whitelist = paste0(\n        c(LETTERS, \" \", \".0123456789-()/\"), collapse = \"\")\n    )\n  )\n  \n  #For Ratings which are just numbers and a decimal point\n  ratings <- tesseract::tesseract(\n    options = list(\n      tessedit_char_whitelist = \"0123456789 .\"\n    )\n  )\n  \n  #Grab the Column starting at Col Start and with width Col with\n  tmp <- processed_image %>% \n    image_crop(geometry_area(col_width, 1009, col_start, 0)) \n  \n  # Run OCR with the correct whitelist and turn into a dataframe\n  tmp %>% \n    ocr(engine = get(format_code)) %>% \n    str_split(\"\\n\") %>%\n    unlist() %>%\n    enframe() %>%\n    select(-name) %>%\n    filter(!is.na(value), str_length(value) > 0)\n}\nThe function above takes in a column width and a column start to crop the column and then a label to choose the whitelist for each specific column. The parameters are defined in a list and passed into purrr’s pmap() function. Finally, all the extracted columns will combined together.\n#Run the function all the various columns\nall_ocr <- list(col_width = c(168, 37, 33, 34, 35, 34),\n                col_start = c(28, 196, 307, 346, 385, 598),\n                format_code = c(\"all_chars\", 'only_chars', rep(\"ratings\", 4))) %>% \n  pmap(ocr_text) \n\n#Combine all the columns together and set the names\nratings <- all_ocr %>% \n  bind_cols() %>% \n  set_names(nm = \"telecast\", \"network\", \"p_18_49\", \"f_18_49\", \"m_18_49\",\n            'p_50_plus') \nFinal Cleaning\nEven with the column specific specifications the ocr() function did not get everything right. Due to the font, it has particular trouble distinguishing between 1s and 4s as well as 8s and 6s. Additionally, sometimes the decimal was still missed. And since all networks were truncated in the original image, I just decided to manually recode.\nratings_clean <- ratings %>% \n  #Fix Things where the decimal was missed\n  mutate(across(p_18_49:p_50_plus, ~parse_number(.x)),\n         across(p_18_49:p_50_plus, ~if_else(.x > 10, .x/100, .x)),\n         #1s and 4s get kindof screwed up; same with 8s and 6s\n         p_50_plus = case_when(\n           telecast == 'TUCKER CARLSON TONIGHT' ~ 2.71,\n           telecast == 'SISTAS SERIES S2' ~ 0.46,\n           telecast == 'LAST WORD W/L. ODONNEL' ~ 2.17,\n           telecast == 'SITUATION ROOM' & p_50_plus == 1.34 ~ 1.31,\n           telecast == 'MY 600-LB LIFE NIA' ~ 0.46,\n           TRUE ~ p_50_plus\n         ),\n         #Clean up 'W/' being read as 'WI' and '11th' as '44th'\n         telecast = case_when(\n           telecast == '44TH HOUR WIB. WILLIAMS' ~ '11TH HOUR W/B. WILLIAMS',\n           telecast == 'ALLIN WI CHRIS HAYES' ~ 'ALL IN W/ CHRIS HAYES',\n           telecast == 'BEAT WIARI MELBER' ~'BEAT W/ARI MELBER',\n           telecast == 'SPORTSCENTER 124M L' ~ 'SPORTSCENTER 12AM',\n           telecast == 'MY 600-LB LIFE NIA' ~ 'MY 600-LB LIFE',\n           TRUE ~ telecast\n         ),\n         # Turn to Title Case\n         telecast = str_to_title(telecast),\n         # Clean up random characters\n         telecast = str_remove(telecast, ' [L|F|S2|L B]+$'),\n         #Clean up Network\n         network = factor(case_when(\n           network == 'TURNI' ~ \"TNT\",\n           network == 'MSNBI' ~ \"MSNBC\",\n           network == 'FOXN' ~ \"FoxNews\",\n           network == 'LIFETI' ~ \"Lifetime\",\n           network == 'BLACK' ~ 'BET',\n           network %in% c('AEN', 'AGEN') ~ 'A&E',\n           network == 'BRAVC' ~ 'BRAVO',\n           network == 'COME' ~ 'COMEDY CENTRAL',\n           network == 'NECS' ~ 'NBC SPORTS',\n           network == 'TBSN' ~ 'TBS',\n           network == 'TL' ~ 'TLC',\n           TRUE ~ network\n         ))\n  )\n\nknitr::kable(head(ratings_clean, 3))\ntelecast\nnetwork\np_18_49\nf_18_49\nm_18_49\np_50_plus\nChallenge Double Agen\nMTV\n0.54\n0.69\n0.39\n0.13\nNba Regular Season\nESPN\n0.33\n0.21\n0.46\n0.40\nAew All Elite Wrestling\nTNT\n0.32\n0.21\n0.42\n0.32\nNow everything should be ready for analysis.\nAnalysis of Cable Ratings\nThe decimals in the table for cable ratings refer to the percent of the population watching the show. For instance the p_18_49 field’s value of 0.54 means that 0.54% of the US 18-49 population watched The Challenge on February 3rd.\nThe Most Popular Shows on Wednesday Night Overall 18-49 and By Gender\nThe first question is what are the most popular shows for the 18-49 demographic for combined genders and broken apart by gender. These types of combined plots uses the patchwork package to combine the three ggplots into a single plot using a common legend.\n##Create Fixed Color Palette For Networks\ncols <- scales::hue_pal()(n_distinct(ratings_clean$network))\nnames(cols) <- levels(ratings_clean$network)\n\n##Top Show By the Key Demo (Combined)\nkey_all <- ratings_clean %>% \n  slice_max(p_18_49, n = 10) %>% \n  ggplot(aes(x = fct_reorder(telecast, p_18_49), y = p_18_49, fill = network)) + \n    geom_col() + \n    geom_text(aes(label = p_18_49 %>% round(2)), nudge_y = 0.015) + \n    scale_y_continuous(expand = expansion(mult = c(0, .1))) + \n    scale_fill_manual(values = cols) + \n    labs(x = \"\", title = \"All Genders\", y = '', fill = '') + \n    coord_flip() + \n    cowplot::theme_cowplot() + \n    theme(\n      axis.text.x = element_blank(),\n      axis.ticks = element_blank(),\n      axis.line.x = element_blank(),\n      plot.title.position = 'plot'\n    )\n\n#Male Ratings only\nkey_male <- ratings_clean %>% \n  slice_max(m_18_49, n = 5) %>% \n  ggplot(aes(x = fct_reorder(telecast, m_18_49), y = m_18_49, fill = network)) + \n  geom_col() + \n  geom_text(aes(label = m_18_49 %>% round(2)), nudge_y = .045) + \n  scale_y_continuous(expand = expansion(mult = c(0, .1))) + \n  scale_fill_manual(values = cols, guide = F) + \n  labs(x = \"\", title = \"Male\", y = '') + \n  coord_flip() + \n  cowplot::theme_cowplot() + \n  theme(\n    axis.text.x = element_blank(),\n    axis.ticks = element_blank(),\n    axis.line.x = element_blank(),\n    plot.title.position = 'plot'\n  )\n\n# Female rating only\nkey_female <- ratings_clean %>% \n  slice_max(f_18_49, n = 5) %>% \n  ggplot(aes(x = fct_reorder(telecast, f_18_49), y = f_18_49, fill = network)) + \n  geom_col() + \n  geom_text(aes(label = f_18_49 %>% round(2)), nudge_y = .065) + \n  scale_y_continuous(expand = expansion(mult = c(0, .1))) + \n  scale_fill_manual(values = cols, guide = F) + \n  labs(x = \"\", title = \"Female\", y = '') + \n  coord_flip() + \n  cowplot::theme_cowplot() + \n  theme(\n    axis.text.x = element_blank(),\n    axis.ticks = element_blank(),\n    axis.line.x = element_blank(),\n    plot.title.position = 'plot'\n  )\n    \n# Combining everything with patchwork syntax\nkey_all / (key_male | key_female) +\n  plot_layout(guides = \"collect\") + \n  plot_annotation(\n    title = \"**Wednesday Night Cable Ratings (Feb 3rd, 2021)**\",\n    caption = \"*Source:* Showbuzzdaily.com\"\n  ) & theme(legend.position = 'bottom',\n            plot.title = ggtext::element_markdown(size = 14),\n            plot.caption = ggtext::element_markdown())\nimgFrom the chart its clear that the Challenge is fairly dominant in the 18-49 Demographic with 0.21% (or 1.63x) higher than the 2nd highest show. Although while the Challenge is popular with both genders its the most popular show among 18-49 Females but only 3rd for 18-49 Males after a NBA game and AEW Professional Wrestling.\nAlso, because the networks for My 600-lb Life (TLC) and Sistas (BET) weren’t in the overall top 10 I couldn’t figure out how to include them in the legend. If anyone has any ideas, please let me know in the comments.\nThe Most Male-Dominant, Female Dominant, and Gender-Balanced Shows\nFrom the above chart its clear that some shows skew Male (sports) and some skew Female (reality shows like Married at First Sight, My 600-lb Life, and Real Housewives). But I can look at that more directly by comparing the ratios the Female 18-49 rating to the Male 18-49 rating to determine the gender skew of each show. I break the shows into categories of Male Skewed, Female Skewed, and Balanced (where the Female/Male Ratio is closest to 1).\n##Female / Male Ratio for Key Demo\nbind_rows(\n  ratings_clean %>% \n    mutate(f_m_ratio = f_18_49 / m_18_49) %>%\n    slice_max(f_m_ratio, n = 5),\n  ratings_clean %>% \n    mutate(f_m_ratio = f_18_49 / m_18_49) %>%\n    slice_min(f_m_ratio, n = 5),\n  ratings_clean %>% \n    mutate(f_m_ratio = f_18_49 / m_18_49,\n           balance = abs(1-f_m_ratio)) %>% \n    slice_min(balance, n = 5)\n) %>%\n  mutate(balance = f_m_ratio-1) %>% \n  ggplot(aes(x = m_18_49, y = f_18_49, fill = balance)) + \n    ggrepel::geom_label_repel(aes(label = telecast)) + \n    geom_abline(lty = 2) + \n    scale_fill_gradient2(high = '#8800FF',mid = '#BBBBBB', low = '#02C2AD',\n                         midpoint = 0, guide = F) + \n    labs(title = \"Comparing 18-49 Demographics by Gender\",\n         subtitle = 'Cable Feb 3rd, 2021',\n         caption = \"*Source:* showbuzzdaily.com\",\n         x = \"Males 18-49 Ratings\",\n         y = \"Females 18-49 Ratings\") + \n    cowplot::theme_cowplot() + \n    theme(\n      plot.title.position = 'plot',\n      plot.caption = ggtext::element_markdown()\n    )\nimgSure enough the most Male dominated shows are sport-related with 2 NBA Games, an NBA pre-game show, an episode of Sportscenter, and a sports talking heads show. Female skewed shows are also not surprising with Married at First Sight, Sistas, My 600-lb Life, and Real Housewives of Salt Lake City topping the list. For the balanced category, I did not have much of an expectation but all the programs seems to be News shows or news adjacent like the Daily Show… which I guess makes sense.\nMost Popular Shows for the 50+ Demographic\nTurning away from the 18-49 demographic I can also look at the most popular shows for the 50+ demographic. Unfortunately, there is not a 50+ gender breakdown so I can only look at the overall.\nratings_clean %>% \n  slice_max(p_50_plus, n = 10) %>% \n  ggplot(aes(x = fct_reorder(telecast, p_50_plus), y = p_50_plus,  fill = network)) + \n  geom_col() + \n  geom_text(aes(label = p_50_plus %>% round(2)), nudge_y = 0.15) + \n  scale_y_continuous(expand = expansion(mult = c(0, .1))) + \n  labs(x = \"\", title = \"Top 10 Cable Shows for the 50+ Demographic\",\n       y = '',\n       subtitle = \"Wednesday, Feb 3rd 2021\",\n       caption = \"*Source:* Showbuzzdaily.com\",\n       fill = '') + \n  coord_flip() + \n  cowplot::theme_cowplot() + \n  theme(\n    axis.text.x = element_blank(),\n    axis.ticks = element_blank(),\n    axis.line.x = element_blank(),\n    plot.title.position = 'plot',\n    plot.caption = ggtext::element_markdown(),\n    legend.position = 'bottom'\n  )\nimgInterestingly in the 50+ Demo, ALL of the shows are News shows and they only come from 3 networks. Two on CNN, Two on Fox News, and 6 on MSNBC. Again, didn’t have a ton of expectation but it was surprising to be how homogeneous the 50+ demographic was.\nThe Oldest and Youngest Shows in the Top 50\nSimilar to the Most Male and Most Female shows in the Top 50 Cable Programs, I’d like to see which shows skew older vs. younger. To do this, I’ll rank order the 18-49 demo and the 50+ demo and plot the ranks against each other. Now there are some massive caveats here in the sense that my data is the Top 50 shows by the 18-49 demo, so its not clear that the 50+ demo is fully represented. Additionally, popularity for each dimension is relative since I don’t know the actual number of people in each demo. Finally, since both scales are ranked, it won’t show the full distance between levels of popularity (e.g, The Challenge is much more popular than the next highest show for 18-49). This was done to produce a better looking visualization.\nI had run a K-means clustering algorithm for text colors to make differences more appearant. There isn’t much rigor to this beyond my assumption that 5 clusters would probably make sense (1 for each corner and 1 middle).\n#Rank Order the Shows for the 2 Columns\ndt <- ratings_clean %>% \n  transmute(\n    telecast,\n    young_rnk = min_rank(p_18_49),\n    old_rnk = min_rank(p_50_plus),\n  ) \n\n# Run K-Means Clustering Algorithm\nkm <- kmeans(dt %>% select(-telecast), \n             centers = 5, nstart = 10)\n\n#Add the cluster label back to the data\ndt2 <- dt %>%\n  mutate(cluster = km$cluster)\n\n#Plot\nggplot(dt2, aes(x = young_rnk, y = old_rnk, color = factor(cluster))) + \n  ggrepel::geom_text_repel(aes(label = telecast), size = 3) +\n  scale_color_discrete(guide = F) + \n  scale_x_continuous(breaks = c(1, 50),\n                     labels = c(\"Less Popular\", \"More Popular\")) + \n  scale_y_continuous(breaks = c(13, 54),\n                     labels = c(\"Less Popular\", \"More Popular\")) + \n  coord_cartesian(xlim = c(-2, 54), ylim = c(0, 52)) + \n  labs(x = \"Popularity Among 18-49\",\n       y = \"Popularity Among 50+\",\n       title = \"Visualizing Popularity of Wednesday Night Cable by Age\",\n       subtitle = \"Comparing 18-49 vs. 50+\") + \n  cowplot::theme_cowplot() + \n  theme(\n    axis.ticks = element_blank(),\n    axis.line = element_blank(),\n    axis.text.y = element_text(angle = 90), \n    panel.background = element_rect(fill = '#EEEEEE')\n\n  )\nimgSomewhat surprising (at least to me), that Rachel Maddow and Tucker Carlson are the consensus most popular shows across the two demos. My beloved Challenge is very popular amongst the 18-49 demo and very unpopular among 50+. Sports shows tended to be generally the least popular by either demo and finally certain MSNBC and Fox News shows were popular among the 50+ demo but not the 18-49.\nConcluding Thoughts\nWhile I still love The Challenge and am happy for its popularity, its best time was probably about 10 years ago (sorry not sorry). As far as the techniques in this post are concerned, I found extracting the data from an image to be an interesting challenge (no pun intended) but if the table was a tractable size I would probably manually enter the data rather than go through this again. Getting the data correct required a lot of guess and check for working with magick and tesseract.\nAs for the analysis, I guess its good when things go as expected (most popular shows by gender follow stereotypical gender conventions) but I think the most surprising thing to me was how much cable news dominated the 50+ Demographic…. and I guess the Daily Show is not as popular as I thought it would be.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-03-11T09:58:40+11:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-03-11-faster-data-exploration-with-dataexplorer/",
    "title": "Faster data exploration with DataExplorer",
    "description": "A short description of the post.",
    "author": [
      {
        "name": "Andrew Treadway",
        "url": "https://theautomatic.net/2021/03/03/faster-data-exploration-with-dataexplorer"
      }
    ],
    "date": "2021-03-11",
    "categories": [],
    "contents": "\nData exploration is an important part of the modeling process. It can also take up a fair amount of time. The awesome DataExplorer package in R aims to make this process easier. To get started with DataExplorer, you’ll need to install it like below:\ninstall.packages``(``\"DataExplorer\"``)\nLet’s use DataExplorer to explore a dataset on diabetes.\n# load DataExplorer\nlibrary(DataExplorer)\n \n# read in dataset\ndiabetes_data <- read.csv(\"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.csv\", header = FALSE)\n \n# fix column names\nnames(diabetes_data) <- c(\"number_of_times_pregnant\", \"plasma_glucose_conc\", \"diastolic_bp\", \"triceps_skinfold_thickness\", \"two_hr_serum_insulin\", \"bmi\", \"diabetes_pedigree_function\", \"age\", \"label\")\n \n# create report\ncreate_report(diabetes_data)\nRunning the create_report line of code above will generate an HTML report file containing a collection of useful information about the data. This includes:\nBasic statistics, such as number of rows and columns, number of columns with missing data, count of continuous variables vs. discrete, and the total memory allocation\nData type for each field\nMissing data percentages for each column\nUnivariate distribution for each column\nQQ plots\nCorrelation analysis\nPCA\nThat’s right – a single line of code can generate all of the above for a given dataset! It’s also possible to get each of these pieces individually. For example, in a single line of code, we can generate histograms for all the numeric variables in the dataset.\nplot_histogram(diabetes_data)\nhistogram dataexplorerSimilarly, we can get bar plots for all categorical variables in the dataset\nplot_bar(diabetes_data)\nHere’s an example getting the correlation plot:\nplot_correlation(diabetes_data)\ncorrelation plot in rConfiguring the report\nIt’s also possible to make adjustments to the output generated by create_report. For example, if you don’t want the QQ plots, you could set add_plot_qq = FALSE\nconfig <- ``configure_report``(add_plot_qq = ``FALSE``)` `create_report``(config = config)\nOne hot encoding\nDataExplorer also comes with a function to perform one hot encoding. You can one hot encode all the categorical variables in the dataset by passing the data frame name to the dummify function. In this case, we don’t have any categorical variables to encode, so the function will generate a warning.\ndummify``(diabetes_data)\n\n\n\n",
    "preview": {},
    "last_modified": "2021-03-11T09:44:51+11:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-03-11-time-series-forecasting-with-xgboost-and-feature-importance/",
    "title": "Time Series Forecasting with XGBoost and Feature Importance",
    "description": "Predicting gold prices in Turkey",
    "author": [
      {
        "name": "Selcuk Disci",
        "url": "https://datageeek.com/2021/03/02/time-series-forecasting-with-xgboost-and-feature-importance/"
      }
    ],
    "date": "2021-03-11",
    "categories": [],
    "contents": "\nTime Series Forecasting with XGBoost and Feature Importance\nThose who follow my articles know that trying to predict gold prices has become an obsession for me these days. And I am also wondering which factors affect the prices. For the gold prices per gram in Turkey, are told that two factors determine the results: USA prices per ounce and exchange rate for the dollar and the Turkish lira. Let’s check this perception, but first, we need an algorithm for this.\nIn recent years, XGBoost is an uptrend machine learning algorithm in time series modeling. XGBoost (Extreme Gradient Boosting) is a supervised learning algorithm based on boosting tree models. This kind of algorithms can explain how relationships between features and target variables which is what we have intended. We will try this method for our time series data but first, explain the mathematical background of the related tree model.\ny_i^t = {k=1}^{K} f{k}(x_i)K represents the number of tree\n represents the basic tree model.\nWe need a function that trains the model by measuring how well it fits the training data. This is called the objective function.\nL= {i} l(, y_i) + {k} (f_k) represents the loss function which is the error between the predicted values and observed values.\n is the regularization function to prevent overfitting.\n(f) = T +  {2} ||w||^2T represents the leaves, the number of leaf nodes of each tree. Leaf node or terminal node means that has no child nodes.\n represents the score (weight) of the leaves of each tree, so it is calculated in euclidean norm.\n represents the learning rate which is also called the shrinkage parameter. With shrinking the weights, the model is more robust against the closeness to the observed values. This prevents overfitting. It is between 0 and 1. The lower values mean that the more trees, the better performance, and the longer training time.\n represents the splitting threshold. The parameter is used to prevent the growth of a tree so the model is less complex and more robust against overfitting. The leaf node would split if the information gain less than . Its range is at \nNow, we can start to examine our case we mention at the beginning of the article. In order to do that, we are downloading the dataset we are going to use, from here.\n#Building data frame\nlibrary(readxl)\n \ndf_xautry <- read_excel(\"datasource/xautry_reg.xlsx\")\ndf_xautry$date <- as.Date(df_xautry$date)\n \n#Splitting train and test data set\ntrain <- df_xautry[df_xautry$date < \"2021-01-01\",]\ntest <- df_xautry[-(1:nrow(train)),]\nWe will transform the train and test dataset to the DMatrix object to use in the xgboost process. And we will get the target values of the train set in a different variable to use in training the model.\n#Transform train and test data to DMatrix form\nlibrary(dplyr)\nlibrary(xgboost)\n \ntrain_Dmatrix <- train %>% \n                 dplyr::select(xe, xau_usd_ounce) %>% \n                 as.matrix() %>% \n                 xgb.DMatrix()\n                 \n \npred_Dmatrix <- test %>% \n                dplyr::select(xe, xau_usd_ounce) %>% \n                as.matrix() %>% \n                xgb.DMatrix()\n \ntargets <- train$xau_try_gram\nWe will execute the cross-validation to prevent overfitting, and set the parallel computing parameters enable because the xgboost algorithm needs it. We will adjust all the parameter we’ve just mentioned above with trainControl function in caret package.\nWe also will make a list of parameters to train the model. Some of them are:\nnrounds: A maximum number of iterations. It was shown by t at the tree model equation. We will set a vector of values. It executes the values separately to find the optimal result. Too large values can lead to overfitting however, too small values can also lead to underfitting.\n*max_depth*: The maximum number of trees. The greater the value of depth, the more complex and robust the model is but also the more likely it would be overfitting.\nmin_child_weight: As we mentioned before with *w* in the objective function, it determines the minimum sum of weights of leaf nodes to prevent overfitting.\n*subsample*: It is by subsetting the train data before the boosting tree process, it prevents overfitting. It is executed once at every iteration.\n#Cross-validation\nlibrary(caret)\n \nxgb_trcontrol <- trainControl(\n  method = \"cv\", \n  number = 10,\n  allowParallel = TRUE, \n  verboseIter = FALSE, \n  returnData = FALSE\n)\n \n#Building parameters set\nxgb_grid <- base::expand.grid(\n  list(\n    nrounds = seq(100,200),\n    max_depth = c(6,15,20), \n    colsample_bytree = 1, \n    eta = 0.5,\n    gamma = 0,\n    min_child_weight = 1,  \n    subsample = 1)\n)\nNow that all the parameters and needful variables are set, we can build our model.\n#Building the model\nmodel_xgb <- caret::train(\n  train_Dmatrix,targets,\n  trControl = xgb_trcontrol,\n  tuneGrid = xgb_grid,\n  method = \"xgbTree\",\n  nthread = 10\n)\nWe can also see the best optimal parameters.\nmodel_xgb$bestTune\n#  nrounds max_depth eta gamma colsample_bytree min_child_weight #subsample\n#1     100         6 0.5     0                1    \nTo do some visualization in the forecast* function, we have to transform the predicted results into the forecast* object.\n#Making the variables used in forecast object \nfitted <- model_xgb %>%\n  stats::predict(train_Dmatrix) %>%\n  stats::ts(start = c(2013,1),frequency = 12)\n \nts_xautrygram <- ts(targets,start=c(2013,1),frequency=12)\nforecast_xgb <- model_xgb %>% stats::predict(pred_Dmatrix)\nforecast_ts <- ts(forecast_xgb,start=c(2021,1),frequency=12)\n \n#Preparing forecast object\nforecast_xautrygram <- list(\n  model = model_xgb$modelInfo,\n  method = model_xgb$method,\n  mean = forecast_ts,\n  x = ts_xautrygram, \n  fitted = fitted,\n  residuals = as.numeric(ts_xautrygram) - as.numeric(fitted)\n)\nclass(forecast_xautrygram) <- \"forecast\"\nWe will show train, unseen, and predicted values for comparison in the same graph.\n#The function to convert decimal time label to wanted format\nlibrary(lubridate)\ndate_transform <- function(x) {format(date_decimal(x), \"%Y\")}\n#Making a time series varibale for observed data\nobserved_values <- ts(test$xau_try_gram,start=c(2021,1),frequency=12)\n \n#Plot forecasting\nlibrary(ggplot2)\nlibrary(forecast)\n \nautoplot(forecast_xautrygram)+\n  autolayer(forecast_xautrygram$mean,series=\"Predicted\",size=0.75) +\n  autolayer(forecast_xautrygram$x,series =\"Train\",size=0.75 ) +\n  autolayer(observed_values,series = \"Observed\",size=0.75) +\n  scale_x_continuous(labels =date_transform,breaks = seq(2013,2021,2) ) +\n  guides(colour=guide_legend(title = \"Time Series\")) +\n  ylab(\"Price\") + xlab(\"Time\") +\n  ggtitle(\"\") +\n  theme_bw()\nimgTo satisfy that curiosity we mentioned at the very beginning of the article, we will find the ratio that affects the target variable of each explanatory variable separately.\n#Feature importance\nlibrary(Ckmeans.1d.dp)\n \nxgb_imp <- xgb.importance(\n  feature_names = colnames(train_Dmatrix),\n  model = model_xgb$finalModel)\n \nxgb.ggplot.importance(xgb_imp,n_clusters = c(2))+ \n  ggtitle(\"\") +\n  theme_bw()+\n  theme(legend.position=\"none\")\n \nxgb_imp$Importance\n#[1] 0.92995147 0.07004853 \nimgConclusion\nWhen we examine the above results and plot, contrary to popular belief, it is seen that the exchange rate has a more dominant effect than the price of ounce gold. In the next article, we will compare this method with the dynamic regression ARIMA model.\nComment from reader:\nA problem with using tree-based models (including xgboost) is that they can’t extrapolate when the predictors are outside the range in the training data (there are some examples of this e.g. in the exercises in http://www.modernstatisticswithr.com/mlchapter.html#boosted-trees). For instance, if you’d only included data up to 2019-12-31 in the training set and then made predictions for 2020, the model wouldn’t predict that the price would continue to increase (try it!). The mobForest package could be a better option here – it fits a random forest with a linear regression in each node, and so can handle extrapolation.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-03-11T09:40:34+11:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-03-10-elvis-omarsar0s-recommendations-for-nlp-resources-via-twitter/",
    "title": "Elvis Saravia's (omarsar0's) recommendations for NLP resources (via Twitter)",
    "description": "List of stuff",
    "author": [
      {
        "name": "Elvis Saravia",
        "url": "https://elvissaravia.substack.com/p/my-recommendations-for-getting-started"
      }
    ],
    "date": "2021-03-10",
    "categories": [],
    "contents": "\nLinks:\nFastAI NLP - https://www.fast.ai/2019/07/08/fastai-nlp/\nStanford cs224n - http://web.stanford.edu/class/cs224n/\nNLP progerss - http://nlpprogress.com/\nCMU neural nets - https://www.youtube.com/playlist?list=PL8PYTP1V4I8AkaHEJ7lOOrlex-pcxS-XV\nModern Deep Learning Techniques Applied to Natural Language Processing - https://nlpoverview.com/\nSpeech and Language Processing (3rd ed. draft) Dan Jurafsky and James H. Martin - https://web.stanford.edu/~jurafsky/slp3/\n\n\n\n",
    "preview": {},
    "last_modified": "2021-03-10T10:02:29+11:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-03-09-deep-learning-in-production-links-from-reddit/",
    "title": "Deep learning in production",
    "description": "Cool links from Reddit",
    "author": [
      {
        "name": "SergiosKar",
        "url": "https://www.reddit.com/r/MachineLearning/comments/m0ew90/d_deep_learning_in_production/"
      }
    ],
    "date": "2021-03-09",
    "categories": [],
    "contents": "\nMachine Learning Infrastructure has been neglected for quite some time by ml educators and content creators. It recently started to gain some traction but the content out there is still limited. Since I believe that it is an integral part of the ML pipeline, I recently finished an article series where I explore how to build, train, deploy and scale Deep Learning models (alongside with code for every post). Feel free to check it out and let me know your thoughts. I am also thinking to expand it into a full book so feedback is much appreciated.\nLaptop set up and system design: https://theaisummer.com/deep-learning-production/\nBest practices to write Deep Learning code: Project structure, OOP, Type checking and documentation: https://theaisummer.com/best-practices-deep-learning-code/\nHow to Unit Test Deep Learning: Tests in TensorFlow, mocking and test coverage: https://theaisummer.com/unit-test-deep-learning/\nLogging and Debugging in Machine Learning: https://theaisummer.com/logging-debugging/\nData preprocessing for deep learning: https://theaisummer.com/data-preprocessing/\nData preprocessing for deep learning (part2): https://theaisummer.com/data-processing-optimization/\nHow to build a custom production-ready Deep Learning Training loop in Tensorflow from scratch: https://theaisummer.com/tensorflow-training-loop/\nHow to train a deep learning model in the cloud: https://theaisummer.com/training-cloud/\nDistributed Deep Learning training: Model and Data Parallelism in Tensorflow: https://theaisummer.com/distributed-training/\nDeploy a Deep Learning model as a web application using Flask and Tensorflow: https://theaisummer.com/deploy-flask-tensorflow/\nHow to use uWSGI and Nginx to serve a Deep Learning model: https://theaisummer.com/uwsgi-nginx/\nHow to use Docker containers and Docker Compose for Deep Learning applications: https://theaisummer.com/docker/\nScalability in Machine Learning: Grow your model to serve millions of users: https://theaisummer.com/scalability/\nIntroduction to Kubernetes with Google Cloud: Deploy your Deep Learning model effortlessly: https://theaisummer.com/kubernetes/\nGithub: https://github.com/The-AI-Summer/Deep-Learning-In-Production\n\n\n\n",
    "preview": {},
    "last_modified": "2021-03-10T09:38:50+11:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-03-09-hello-world/",
    "title": "Hello, world!",
    "description": "Instructions for using Distill as a notepad",
    "author": [
      {
        "name": "Darya Vanichkina",
        "url": "daryavanichkina.com"
      }
    ],
    "date": "2021-03-09",
    "categories": [],
    "contents": "\nIn this “post”, I’d like to offer details (mostly for myself) on how to get this to work. I save a lot of links to materials/blog posts/RWeekly/RBloggers posts, and instead of reading them, taking notes and forgetting them I’d like to instead use this space to host them as, effectively, a public Evernote/Bear/Quiver/notetaking tool of choice.\nWhen you want to save a blog post from the interwebs:\nOpen this project\nRun distill::create_post(“The random post title”)\nThis will create and Rmd document for the post.\nEdit the metadata, especially author, original URL and citation_url; the last one will make the citations at the bottom of the page link to the author’s original post, not mine. Then open the .Rmd in Typora.\nCopy/paste the content into Typora. Make sure you correctly set the image file paths to match what Rmd expects.\nKnit the document in Rstudio. Make sure to add to any R code chunks that have been copy/pasted.\nBuild the website in the Build tab in Rstudio.\nPush to GitHub.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-03-10T09:38:24+11:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-03-09-how-to-give-a-status-update-to-executives/",
    "title": "How to Give a Status Update To Executives",
    "description": "A story and a framework",
    "author": [
      {
        "name": "Jacob Kaplan-Moss",
        "url": "https://jacobian.org/2021/mar/5/exec-status-update/"
      }
    ],
    "date": "2021-03-09",
    "categories": [],
    "contents": "\nHere’s a weird little skill I had to learn the hard way: how to give a status update to executives, investors, or boards.\nIf you’ve been in any sort of project lead position, you’ve probably been asked for a status. In most circumstances, specifics are details are pretty important – e.g., if your boss is asking for an update, they probably want to know what’s up in some depth. The good, the bad, and the ugly – you want to share all of it.\nThis changes once the audience is an executive team, and that can come as a surprise (it certainly wasn’t something I was prepared for). Executives tend to have a huge number of projects and priorities they’re paying attention to. And, the dynamic of briefing a team means each of them has their own different set of priorities and projects.\nSo, when briefing this specific audience, the format needs to change. Instead of specifics and details, the focus needs to be only on what you think this team needs to know. A good executive status update is very short – a few minutes, max – and includes just a handful of key points. More, and you risk misunderstanding.\nHere’s my structure for this kind of update:\nAn overall summary about how the project is going. I’ll usually first share how I’m feeling about the project (“it’s running well” / “I’m worried about …”) to anchor what comes next, but I’ll quickly aim for something quantitative (e.g., “we’re two weeks ahead of schedule”) to put that feeling into context.\nOne or two highlights, if appropriate. If we just hit a big milestone, or shipped something major, or solved a huge problem, it’s worth bragging about. I’ll also use this point to highlight someone on the team whose work has been especially great lately.\nOne or two of our biggest risks, if they’re something the executive team needs to know about. If we’re over budget, running late, or at risk of any of those: I’ll highlight them.\nA request for the team (or specific people), if I have it.\n“Any questions?”\nHere’s what this might sound like:\n\nProject X is running smoothly. We’re making steady progress: we’ve delivered a bit over half of the features on our roadmap, and we’re on track to launch publicly in May.\nI want to particularly highlight J—’s design work; every time we share a new demo with our user research group they rave over how much they love the design.\nWe do have some cost risk: right now, the code’s pretty inefficient and would require us to increase our AWS spend by 25% when we put this into production. We either need to decide that cost is acceptable, or add some extra time to the schedule for performance optimization. I need some guidance from this team on that point: can you folks let me know if that cost seems OK or not?\nThanks, any questions?\n\nThis might seem weird: there aren’t any specifics about deliverables, or features shipped, or how anyone on the team is doing (other than J—), etc. But it’s exactly what I want the executive team to know: things are on track, one person on the team is doing great work worth highlighting, and here’s my one question.\nI think people can struggle with this lack of detail when they first try it. It can seem like there’s not enough information to show the status accurately. Or they’re afraid that the lack of detail can be seen as a sign that they don’t know what’s going on. They can feel the need to provide “proof” by getting into the weeds.\nI think the difference is that, at the top of an organization, there tends to be a shift away from this kind of “proof” towards a general assumption of information-sharing on a need-to-know basis. Executives, especially at large organizations, need to synthesize a truly staggering amount of information, and so effective executives learn to value people who filter for “what you need to know.” The assumption becomes: if you didn’t mention it it’s because it doesn’t warrant mention, not because you don’t know it.\nOf course, you still need †o know your shit! There very well might be questions, and detailed ones, and if you can’t back up what you’ve just said with the details, it’s a bad look. So the preparation for a status update is the same as any other situation, you just edit judiciously before actually speaking.\nStory time: here’s how I learned this lesson:\nI was leading incident response after a particularly nasty security breach, and I was asked to give a status update to the executive team at their next meeting. This was a large company – over 10,000 employees – and it was the first time I’d met any of the executives.\nI had about a week to prepare, and I spent most of it pulling together a ton of data and information into a briefing. I think I ended up with about two dozen slides.\nThe day before the meeting, I got an email from the assistant to the CEO. He wrote, “you’ll give your update from 1:35 - 1:38, followed by 2 minutes for questions”. I thought for sure this was a typo, but no: I had 180 seconds for my update about an incident response that had been running for several weeks and included dozens of staff and at least one law enforcement agency.\nIn the end it went well: I was able to get some emergency coaching to help distill my points down, and that’s where I started to develop the template that you see above. But, gosh, that day was terrifying; I wish I’d been able to learn this skill in a less stressful way!\n\n\n\n",
    "preview": {},
    "last_modified": "2021-03-10T09:33:53+11:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-03-09-powerpoint-slides-in-r-via-officer-a-handholding-guide/",
    "title": "PowerPoint Slides in R via officer: A handholding guide",
    "description": "How to make ppt from R",
    "author": [
      {
        "name": "Ljupcho Naumov",
        "url": "https://ljupcho.com/blog/powerpoint"
      }
    ],
    "date": "2021-03-09",
    "categories": [
      "Word"
    ],
    "contents": "\nPowerPoint Slides in R via officer: A handholding guide\nThe officer library, while powerful, is not easy to figure out. This guide aims to give you all the gotchas to create a professional template in PowerPoint that can be filled in with content from R or shiny.\nThe 3 gotchas are:\nThe template slides need to be made as slide masters, not actual slides.\nThe content placeholders need to be actual PowerPoint placeholders found in the Master slide view, and not textboxes.\nYou can change the placeholder labels with Alt + F10. Alternatively, Home > Arrange > Selection Pane. These labels will serve as an ID for the placeholders so you can refer to them in the code.\nExample: Stock Summary Slide\nThe example I’ll be holding you hand through will be a stock presentation. We’ll use data for public companies to create a 1 page stock summary for a given stock ticker. The only user input to the code is the stock ticker. It will look like this. You can see all the code and presentations already generated for companies like Apple, Berkshire and Gamestop on the Github.\nimg1. Open a new Powerpoint file and Delete All Master Templates\nOpen the Master slides by going to View > Slide Master. Delete everything. Leave nothing behind. On the one master powerpoint won’t let you delete, delete all the elements. Now you should have a blank canvas to design your slide templates. If you already have a company slide template, you can copy that here. Be sure to name every slide template with Right Click > Rename Layout.\n2. Place your content placeholders and design your slide\nNow’s the time for the first two gotchas. You create your template within this Master Slide, and for placeholders you use the Powerpoint given ones, not textboxes or other shapes. The placeholders are found Slide Master > Insert Placeholder > Text. You can use the other placeholder types as well, but as far as I know the placeholder type doesn’t matter for officer. In the placeholder you can type in text to remind yourself what content goes where, but also to format the final text. Formatting options such as font, size and color will be applied on the text plugged in from R. Pro tip: If you want the text to fully fit the defined placeholder remove the placeholder margins which Powerpoint automatically adds. Right click on the placeholder > Format Shape > Text Options > Click on the 3rd Icon > Set all the margins down to 0cm.\n3. Label your Placeholders\nThe labels will serve as placholder IDs so that we can refer to them with officer. Once the placeholders are in place, click Alt + F10 to access the element label menu. Here you can see that PowerPoint has given the elements useless labels like Text Placeholder 3, and if you’ve copy pasted placeholders be sure that PowerPoint has duplicated the label as well. By doubleclicking the labels you can change them into something more identifiable. With these labels you’ll tell officer what content to put where.\n4. Open R and see how officer percieves the template\nWith the placeholders labeled we leave powerpoint for the comfort of R. First we load officer and read in the powerpoint template.\n\n\nlibrary(tidyverse)\nlibrary(officer)\nlibrary(yfinance)\n\nread_pptx(\"stock_summary_template.pptx\") %>% officer::layout_properties()\n\n\n\nlayout_properties() shows you how Officer sees the elements you’ve just defined. master_name is the name of the Slide master (the very first slide you see on Slide Master view) and name is the slide name (defined in step 1). type and id we won’t use. ph_label is the most important column and here you should see the labels defined in step 3. The rest of the columns are dimensions of the elements, offx and offy are the x and y coordinates of the top left corner of the element, and cx and cy are the width and length of the elements in inches. These dimensions, especially width(cx) will be especially useful when creating images or tables for your presentation using the flextable package, where heights and widths have to be defined. That way you can define the placeholder by hand in powerpoint, get the width and height, and create elements in R that exactly fit the placholder space.\n5. Fill out the template\n​ Now that we have the template loaded, it’s time to create the content and place it in the powerpoint. In our presentation we’ll have a company description (text), 12 months price result (image), and Sales Growth and EBIT margin charts (ggplot). First let’s load the libraries. The only custom function I have is a function to get and calculate the width and height of placeholders.\n\n\n# Libraries and functions\nlibrary(tidyverse)\nlibrary(officer)\nlibrary(quantmod)\nlibrary(yfinance)\nlibrary(ggdark)\nlibrary(rvg)\n\nget_placeholder_dims <- function(presentation, label, dim = \"width\", unit = \"in\") {\n   layout <- officer::layout_properties(presentation)\n   ifelse(\n     dim==\"width\",\n     dimension <- layout$cx[match(label, layout$ph_label)],\n     dimension <- layout$cy[match(label, layout$ph_label)]\n   )\n\n   if (unit == \"in\") {\n     dimension\n   } else if (unit == \"px\") {\n     dimension*96\n   } else if (unit == \"cm\") {\n     dimension * 2.54\n   } else stop(glue::glue(\"Only 'in', 'px' and 'cm' are supported as units. You entered '{unit}'.\"))\n}\n\n\n\nLet’s start at the ending: how the content plugs into the powerpoint.\n\n  my_pres <- read_pptx(\"stock_summary_template.pptx\") %>%   # reads in the tempalate\n        remove_slide(index = 1) %>%                         # deletes the first slide. All presentations must have at lease one actual slide to be saved.\n        add_slide(layout = \"Company Summary\", master = \"Office Theme\") %>%    # we add a slide according to the layout we have named \"Company Summary\" which is under the \"Office Theme\" master\n        ph_with(value = company_name, location = ph_location_label(ph_label = \"Company Name\")  ) %>%   # put the string company_name in the Company Name placeholder\n        ph_with(value = ticker location = ph_location_label(ph_label = \"Ticker\")  ) %>%               # put ticker in the ticker placeholder\n        ph_with(value = glue::glue(\"Sector: {summary$sector}\") , location = ph_location_label(ph_label = \"Sector\")  ) %>%\n        ph_with(value = glue::glue(\"Industry: {summary$industry}\"), location = ph_location_label(ph_label = \"Industry\")  ) %>%  # put the sector and industry strings in their respective placeholders\n        ph_with(value = summary$longBusinessSummary, location = ph_location_label(ph_label = \"Company Summary\")  ) %>%   # put the summary ...\n        ph_with(value = external_img(ttm_performance_path), location = ph_location_label(ph_label = \"TTM Perfromance\")  ) %>%  # HARD PART, look below: take the image located at ttm_performance_path and put it at the TTM Perfromance placeholder\n        ph_with(value = rvg::dml(ggobj = sales_growth, bg = \"transparent\"), location = ph_location_label(ph_label = \"Sales Growth\")  ) %>%   # take the ggplot object sales_growth and place it. rvg::dml is magic we'll discuss below.\n        ph_with(value = rvg::dml(ggobj = ebit_margin, bg = \"transparent\"), location = ph_location_label(ph_label = \"EBIT Margin\")  )   # Same, take the ggplot object ebit_margin and place it\n\nprint(my_pres, glue::glue(\"presentations/{company_name} Summary {Sys.Date()}.pptx\")) # Finally print the presentation with the commpany name at a given location. DONE!\n\nNow let’s create the hard content. The string based content is easy, basically put the string in value, and define the location with ph_location_label. For the hard part let’s start with a chart for the 12 Month perfromance of the stock. As officer doesn’t natively support chart objects from quantmod, we’ll convert the output into an image that we’ll plug in the powerpoint. In the code below I have explanations as comments.\n\n  presentation <- read_pptx(\"stock_summary_template.pptx\") # let's read in the template as we did\n\n  ticker <- \"AMZN\" # the only input, the stock ticker\n\n  company_name <- yfinance::get_company_names(ticker)\n  summary <- yfinance::get_summaries(ticker) # here we have a business symmary, as well as data on the sector and industry of the company\n\n  ttm_performance_path <- glue::glue(\"presentations/images/{ticker} TTM Chart.png\") # The location where we want to save the image. To plug in to the Powerpoint, we use this path along with external_img().\n\n  prices <- getSymbols(ticker from=Sys.Date()-365, to = Sys.Date(), auto.assign = FALSE) # get the price and volatility data from the stock\n\n  png(filename = ttm_performance_path,\n      width = get_placeholder_dims(presentation, \"TTM Perfromance\", dim =\"width\", unit = \"px\"),\n      height = get_placeholder_dims(presentation, \"TTM Perfromance\", dim = \"hight\", unit = \"px\")\n      )\n# this sets up the image location as well as the width and height in pixels. Using my function, it can pull the data directly from the powerpoint placholder in order to create an image to perfectly fit the predefined area.\n\n\n  chartSeries(prices, name = glue::glue(\"{ticker} - 12 Months\"), type=\"line\") # this is the actual chart\n\n  dev.off() # this tells R to save the chart with the previous settings\n\nNext are two ggplots. When we add them to the presentation we wrap them in rvg::dml(). This converts the ggplot into a vector graphic, such that every element of it, like title, axis names, even line colors can be changed directly in Powerpoint. Also resizes don’t skew the image proportions. We could have provided directly the ggplot objects, but the final plots would have been images that get distorted if you change only the width for instance. Using rvg::dml() gives as much flexibility to do finishing touches in Powerpoint as if you made the chart natively in Office.\n\n\n  ebit_margin <-\n  get_income(ticker) %>%\n  mutate(\n    ebit_margin = ebit / totalRevenue\n  ) %>%\n  ggplot()+\n  geom_bar(mapping = aes(x = date, y = ebit_margin), stat = \"identity\", fill = \"#5efc82\", width = 0.4)+\n  geom_text(\n    mapping = aes(x = date, y = ebit_margin, label = paste0(round(ebit_margin*100, 2), \"%\")),\n    vjust = -0.2\n  )+\n  scale_y_continuous(labels = scales::percent_format(accuracy = 1))+\n  labs(title = \"EBIT Margin\", x = \"Date\", y = \"EBIT Margin\")+\n  dark_theme_gray()+\n  theme(\n    text = element_text(size = 11),\n    plot.background =element_blank(),\n    panel.background = element_blank(),\n    panel.grid.major = element_line(color = \"grey30\", size = 0.2),\n    panel.grid.minor = element_blank()\n  )\n\n  sales_growth <-\n    get_income(ticker) %>%\n    arrange(date) %>%\n    mutate(\n      sales_growth = totalRevenue / lag(totalRevenue) - 1\n    ) %>%\n    dplyr::filter(!is.na(sales_growth)) %>%\n    ggplot()+\n    geom_bar(mapping = aes(x = date, y = sales_growth), stat = \"identity\", fill = \"#6200ea\", width = 0.4)+\n    geom_text(\n      mapping = aes(x = date, y = sales_growth, label = paste0(round(sales_growth*100, 2), \"%\")),\n      vjust = -0.2\n    )+\n    scale_y_continuous(labels = scales::percent_format(accuracy = 1))+\n    labs(title = \"Sales Growth\", x = \"Date\", y = \"Sales Growth\")+\n    dark_theme_gray()+\n    theme(\n      text = element_text(size = 11),\n      plot.background = element_blank(),\n      panel.background = element_blank(),\n      panel.grid.major = element_line(color = \"grey30\", size = 0.2),\n      panel.grid.minor = element_blank()\n    )\n\n\n\nAll of the code together:\n\n\n  library(tidyverse)\n  library(officer)\n  library(quantmod)\n  library(yfinance)\n  library(ggdark)\n  library(rvg)\n\n\n  get_placeholder_dims <- function(presentation, label, dim = \"width\", unit = \"in\") {\n     layout <- officer::layout_properties(presentation)\n     ifelse(\n       dim==\"width\",\n       dimension <- layout$cx[match(label, layout$ph_label)],\n       dimension <- layout$cy[match(label, layout$ph_label)]\n     )\n\n     if (unit == \"in\") {\n       dimension\n     } else if (unit == \"px\") {\n       dimension*96\n     } else if (unit == \"cm\") {\n       dimension * 2.54\n     } else stop(glue::glue(\"Only 'in', 'px' and 'cm' are supported as units. You entered '{unit}'.\"))\n  }\n\n\n\n\npresentation <- read_pptx(\"stock_summary_template.pptx\")\n\nticker <- \"AMZN\"\n\ncompany_name <- yfinance::get_company_names(ticker)\nsummary <- yfinance::get_summaries(ticker)\n\nttm_performance_path <- glue::glue(\"presentations/images/{ticker} TTM Chart.png\")\n\nprices <- getSymbols(ticker from=Sys.Date()-365, to = Sys.Date(), auto.assign = FALSE)\n\npng(filename = ttm_performance_path,\n    width = get_placeholder_dims(presentation, \"TTM Perfromance\", dim =\"width\", unit = \"px\"),\n    height = get_placeholder_dims(presentation, \"TTM Perfromance\", dim = \"hight\", unit = \"px\")\n    )\nchartSeries(prices, name = glue::glue(\"{ticker} - 12 Months\"), type=\"line\")\n\ndev.off()\n\nebit_margin <-\n  get_income(ticker) %>%\n  mutate(\n    ebit_margin = ebit / totalRevenue\n  ) %>%\n  ggplot()+\n  geom_bar(mapping = aes(x = date, y = ebit_margin), stat = \"identity\", fill = \"#5efc82\", width = 0.4)+\n  geom_text(\n    mapping = aes(x = date, y = ebit_margin, label = paste0(round(ebit_margin*100, 2), \"%\")),\n    vjust = -0.2\n  )+\n  scale_y_continuous(labels = scales::percent_format(accuracy = 1))+\n  labs(title = \"EBIT Margin\", x = \"Date\", y = \"EBIT Margin\")+\n  dark_theme_gray()+\n  theme(\n    text = element_text(size = 11),\n    plot.background =element_blank(),\n    panel.background = element_blank(),\n    panel.grid.major = element_line(color = \"grey30\", size = 0.2),\n    panel.grid.minor = element_blank()\n  )\n\n\nsales_growth <-\n  get_income(ticker) %>%\n  arrange(date) %>%\n  mutate(\n    sales_growth = totalRevenue / lag(totalRevenue) - 1\n  ) %>%\n  dplyr::filter(!is.na(sales_growth)) %>%\n  ggplot()+\n  geom_bar(mapping = aes(x = date, y = sales_growth), stat = \"identity\", fill = \"#6200ea\", width = 0.4)+\n  geom_text(\n    mapping = aes(x = date, y = sales_growth, label = paste0(round(sales_growth*100, 2), \"%\")),\n    vjust = -0.2\n  )+\n  scale_y_continuous(labels = scales::percent_format(accuracy = 1))+\n  labs(title = \"Sales Growth\", x = \"Date\", y = \"Sales Growth\")+\n  dark_theme_gray()+\n  theme(\n    text = element_text(size = 11),\n    plot.background = element_blank(),\n    panel.background = element_blank(),\n    panel.grid.major = element_line(color = \"grey30\", size = 0.2),\n    panel.grid.minor = element_blank()\n  )\n\n\nmy_pres <- presentation %>%\n        remove_slide(index = 1) %>%\n        add_slide(layout = \"Company Summary\", master = \"Office Theme\") %>%\n        ph_with(value = company_name, location = ph_location_label(ph_label = \"Company Name\")  ) %>%\n        ph_with(value = ticker location = ph_location_label(ph_label = \"Ticker\")  ) %>%\n        ph_with(value = glue::glue(\"Sector: {summary$sector}\") , location = ph_location_label(ph_label = \"Sector\")  ) %>%\n        ph_with(value = glue::glue(\"Industry: {summary$industry}\"), location = ph_location_label(ph_label = \"Industry\")  ) %>%\n        ph_with(value = summary$longBusinessSummary, location = ph_location_label(ph_label = \"Company Summary\")  ) %>%\n        ph_with(value = external_img(ttm_performance_path), location = ph_location_label(ph_label = \"TTM Perfromance\")  ) %>%\n        ph_with(value = rvg::dml(ggobj = sales_growth, bg = \"transparent\"), location = ph_location_label(ph_label = \"Sales Growth\")  ) %>%\n        ph_with(value = rvg::dml(ggobj = ebit_margin, bg = \"transparent\"), location = ph_location_label(ph_label = \"EBIT Margin\")  )\n\nprint(my_pres, glue::glue(\"presentations/{company_name} Summary {Sys.Date()}.pptx\"))\n\nThere you have it. You can find the code also on Github. I’m sure I’ve dropped your hand at some point, so to drive the (power)point across, I’ll be making a youtube tutorial as well. Stay tuned and feel free to reach out with questions.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-03-10T09:39:13+11:00",
    "input_file": {}
  },
  {
    "path": "posts/welcome/",
    "title": "Welcome to Darya's public pocket",
    "description": "This is a public note space",
    "author": [
      {
        "name": "Darya Vanichkina",
        "url": "daryavanichkina.com"
      }
    ],
    "date": "2021-03-09",
    "categories": [],
    "contents": "\n\n\n\n",
    "preview": {},
    "last_modified": "2021-03-09T12:54:30+11:00",
    "input_file": {}
  }
]
